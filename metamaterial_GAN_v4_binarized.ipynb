{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th Version of Metamaterials GAN\n",
    "Beginning by following MNIST 'template', then adding complexity as problem dictates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "# useful v1 functions\n",
    "import import_ipynb \n",
    "import importlib\n",
    "import metamaterials_GAN_v1\n",
    "importlib.reload(metamaterials_GAN_v1)\n",
    "\n",
    "from metamaterials_GAN_v1 import plot_shape, load_item, quarter, dataset, dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1098910, 1, 32, 32])\n",
      "34340.9375\n"
     ]
    }
   ],
   "source": [
    "print(dataset.waveguides.size())\n",
    "print(1098910/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator for conditional GAN\n",
    "    Inputs:\n",
    "        - Waveguide, size: (batch_size, 1, 32, 32)\n",
    "        - Parameters, size (batchsize, 4)\n",
    "        - Modes (condition), size (batchsize, 4)\n",
    "    Outputs:\n",
    "        - 0-1, if image is real or generated, size (batchsize)\n",
    "    Questions:\n",
    "        - Should I be using dropout in image_fc, or at all in my Discriminator??\n",
    "        - Am I correct in using conv2d and splitting the problem into\n",
    "          image convolution and parameter process and then combining?\n",
    "        - \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Process for waveguide\n",
    "        # Note on conv, output_size = 1 + [(input_size + 2*padding-kernel_size)/stride]\n",
    "        self.image_conv = nn.Sequential(\n",
    "            # Input is an image of shape (1,32,32), meaning greyscale and 32x32 pixels\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), # (batchsize, 64, 16, 16) -> 65 channels, each of size 16 x 16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # (batchsize, 128, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # (batchsize, 256, 4, 4)\n",
    "            nn.Flatten() #  (batchsize, 256 x 4 x 4 = 4096) for linear output\n",
    "        )\n",
    "\n",
    "        # Process for parameters\n",
    "        self.param_fc = nn.Sequential(\n",
    "            # Need to take (batchsize, 4) and make (batchsize, 256) for concatenation,\n",
    "            # add hidden layer so that we can infer information about parameters as well.\n",
    "            nn.Linear(4,128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "        # Process for modes\n",
    "        self.cond_fc = nn.Sequential(\n",
    "            # Rescales (batchsize, 4->256), maps to same feature space as image and params\n",
    "            nn.Linear(8, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Full combined model for all processes\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4096 + 256 + 256, 512), # image + params + cond\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, params, cond):\n",
    "        # img_bin = torch.where(img > 0.5, 1.0, 0.0)\n",
    "        img_feat = self.image_conv(img)\n",
    "        param_feat = self.param_fc(params)\n",
    "        cond_feat = self.cond_fc(cond)\n",
    "\n",
    "        x = torch.cat([img_feat, param_feat, cond_feat], dim=1)\n",
    "        final = self.model(x)\n",
    "\n",
    "        return final.squeeze() # returns (batchsize), where each number is 0 -> 1 based on how likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator for conditional GAN\n",
    "    Inputs:\n",
    "        - Modes (condition), size (batchsize, 4)\n",
    "    Outputs:\n",
    "        - Waveguide, size (batchsize, 32, 32)\n",
    "        - Params, size (batchsize, 4)\n",
    "    Questions:\n",
    "        - Should we still be using latent vector like in MNIST, as we want \n",
    "          consistent results i.e. for a set of modes, we want as close \n",
    "          to the same waveguide as possible each time? \n",
    "        - Should I be feeding my generated waveguide shape into my params\n",
    "          process as well (and maybe in discrim too)? Also, does my params\n",
    "          process need more layers?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            # Need to transform cond vector into higher dimension\n",
    "            # so that we can reshape it for deconv (batchsize, 8 -> 4096)\n",
    "            nn.Linear(8, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(inplace=True) \n",
    "        )\n",
    "        # Output = (input_size-1)*stride-2*padding+kernel_size\n",
    "        self.deconv = nn.Sequential(\n",
    "            # We start with 256 4x4 pieces generated from our cond input\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 4x4 ->  8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), # 16x16 -> 32x32, greyscale so only 1 output channel\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Takes in cond and outputs parameters\n",
    "        \n",
    "        self.param_proc = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 4) # outputs (batchsize, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        x = self.fc(cond) # (batchsize, 8 -> 4096)\n",
    "        cond_feat = x.view(x.size(0), 256, 4, 4) #(batchsize, 4096) -> (batchsize, 256, 4, 4)\n",
    "        image = self.deconv(cond_feat)\n",
    "        params = self.param_proc(x)\n",
    "\n",
    "        return image, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch, discriminator, generator, g_optimizer, g_criterion, d_criterion, device, adv_w=1, re_w=1):\n",
    "    generator.train()\n",
    "    g_optimizer.zero_grad()\n",
    "    # will almost certainly have to change but same logic flow\n",
    "    eigenmodes, weights, real_params, real_waveguides = [b.to(device) for b in batch]\n",
    "    cond = torch.cat([eigenmodes, weights], dim=-1)\n",
    "\n",
    "    fake_waveguides, fake_params = generator(cond)\n",
    "\n",
    "    validity = discriminator(fake_waveguides, fake_params, cond)\n",
    "    adv_loss = d_criterion(validity, Variable(torch.ones_like(validity))) # how it fairs against discriminator\n",
    "\n",
    "    # These are how it fairs against real data, included because only one real result, unsure if to keep? \n",
    "    image_loss = g_criterion(fake_waveguides, real_waveguides)\n",
    "    params_loss = g_criterion(fake_params, real_params)\n",
    "\n",
    "    # can adjust weights to make it fully adversarial \n",
    "    g_loss = adv_loss * adv_w + (image_loss + params_loss) * re_w\n",
    "\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch, discriminator, generator, d_optimizer, d_criterion, device):\n",
    "    discriminator.train()\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    eigenmodes, weights, real_params, real_waveguides = [b.to(device) for b in batch]\n",
    "    cond = torch.cat([eigenmodes, weights], dim=-1)\n",
    "\n",
    "    real_validity = discriminator(real_waveguides, real_params, cond)\n",
    "    real_loss = d_criterion(real_validity, Variable(torch.ones_like(real_validity)))\n",
    "\n",
    "    fake_waveguides, fake_params = generator(cond)\n",
    "    fake_validity = discriminator(fake_waveguides, fake_params, cond)\n",
    "    fake_loss = d_criterion(fake_validity, Variable(torch.zeros_like(real_validity)))\n",
    "\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out differences between my dataset structure and MNIST dataset structure\n",
    "\n",
    "Need to implement training loop, remember that output must be binarized before being fed to the discriminator!\n",
    "\n",
    "For binarization, will that not significantly increase the loss of my model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0... Done! - 425.06715688900294 s\n",
      "Starting epoch 1... Done! - 431.54990874199575 s\n",
      "Starting epoch 2... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#for _ in range(n_critic):\u001b[39;00m\n\u001b[32m     30\u001b[39m d_loss = discriminator_train_step(batch, d,g, d_optimizer, d_criterion, device)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m g_loss = \u001b[43mgenerator_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_w\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_w\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# writer.add_scalars('scalars', {'g_loss': g_loss, 'd_loss': (d_loss / n_critic)}, step)  \u001b[39;00m\n\u001b[32m     34\u001b[39m writer.add_scalars(\u001b[33m'\u001b[39m\u001b[33mscalars\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mg_loss\u001b[39m\u001b[33m'\u001b[39m: g_loss, \u001b[33m'\u001b[39m\u001b[33md_loss\u001b[39m\u001b[33m'\u001b[39m: (d_loss)}, step)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mgenerator_train_step\u001b[39m\u001b[34m(batch, discriminator, generator, g_optimizer, g_criterion, d_criterion, device, adv_w, re_w)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# can adjust weights to make it fully adversarial \u001b[39;00m\n\u001b[32m     18\u001b[39m g_loss = adv_loss * adv_w + (image_loss + params_loss) * re_w\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mg_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m g_optimizer.step()\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m g_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # set up device\n",
    "\n",
    "from datetime import datetime\n",
    "d = Discriminator().to(device)\n",
    "g = Generator().to(device) \n",
    "d_optimizer = torch.optim.Adam(d.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(g.parameters(), lr=1e-4)\n",
    "d_criterion = nn.BCELoss() # outputs [0,1]\n",
    "g_criterion = nn.MSELoss() # outputs [-1,1]\n",
    "\n",
    "run_name = f\"WGAN-lr1e4-bs32-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"runs/gan_experiments_only_adv_/{run_name}\")\n",
    "num_epochs = 50\n",
    "# n_critic = 5\n",
    "display_step = 1000\n",
    "save_path = 'models/generator_v4_only_adv.pth' \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    print('Starting epoch {}...'.format(epoch), end=' ')\n",
    "    i = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        step = epoch * len(dataloader) + i + 1\n",
    "        i += 1\n",
    "\n",
    "        #for _ in range(n_critic):\n",
    "        d_loss = discriminator_train_step(batch, d,g, d_optimizer, d_criterion, device)\n",
    "        \n",
    "        g_loss = generator_train_step(batch, d, g, g_optimizer, g_criterion, d_criterion, device, adv_w=1, re_w=0)\n",
    "        # writer.add_scalars('scalars', {'g_loss': g_loss, 'd_loss': (d_loss / n_critic)}, step)  \n",
    "        writer.add_scalars('scalars', {'g_loss': g_loss, 'd_loss': (d_loss)}, step)\n",
    "        # print(step)\n",
    "        if step % display_step == 0:\n",
    "            g.eval()\n",
    "            batch = [dataset[i] for i in range(10)]\n",
    "            e_modes, weights, params, real_wguides = zip(*batch)\n",
    "\n",
    "            e_modes = torch.stack(e_modes).to(device)          # (10, 4)\n",
    "            weights = torch.stack(weights).to(device)          # (10, 4)\n",
    "            cond = torch.cat([e_modes, weights], dim=1)        # (10, 8)\n",
    "\n",
    "            real_wguides = torch.stack(real_wguides).to(device)  # (10, 1, 32, 32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_wguides, _ = g(cond)                       # (10, 1, 32, 32)\n",
    "            grid_fake = make_grid(fake_wguides, nrow=5, normalize=True)\n",
    "            grid_real = make_grid(real_wguides, nrow=5, normalize=True)\n",
    "\n",
    "            # Write to TensorBoard\n",
    "            writer.add_image(f'gan_experiments_only_adv_/{run_name}/Generated_Waveguides', grid_fake, step)\n",
    "            writer.add_image(f'gan_experiments_only_adv_/{run_name}/Real_Waveguides', grid_real, step)\n",
    "            writer.flush()\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f'Done! - {elapsed} s')\n",
    "    torch.save(g.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
