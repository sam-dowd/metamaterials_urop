{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th Version of Metamaterials GAN\n",
    "Beginning by following MNIST 'template', then adding complexity as problem dictates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "# useful v1 functions\n",
    "import import_ipynb \n",
    "import importlib\n",
    "import metamaterials_GAN_v1\n",
    "importlib.reload(metamaterials_GAN_v1)\n",
    "\n",
    "from metamaterials_GAN_v1 import plot_shape, load_item, quarter, dataset, dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator for conditional GAN\n",
    "    Inputs:\n",
    "        - Waveguide, size: (batch_size, 1, 32, 32)\n",
    "        - Parameters, size (batchsize, 4)\n",
    "        - Modes (condition), size (batchsize, 4)\n",
    "    Outputs:\n",
    "        - 0-1, if image is real or generated, size (batchsize)\n",
    "    Questions:\n",
    "        - Should I be using dropout in image_fc, or at all in my Discriminator??\n",
    "        - Am I correct in using conv2d and splitting the problem into\n",
    "          image convolution and parameter process and then combining?\n",
    "        - \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Process for waveguide\n",
    "        # Note on conv, output_size = 1 + [(input_size + 2*padding-kernel_size)/stride]\n",
    "        self.image_conv = nn.Sequential(\n",
    "            # Input is an image of shape (1,32,32), meaning greyscale and 32x32 pixels\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), # (batchsize, 64, 16, 16) -> 65 channels, each of size 16 x 16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # (batchsize, 128, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # (batchsize, 256, 4, 4)\n",
    "            nn.Flatten() #  (batchsize, 256 x 4 x 4 = 4096) for linear output\n",
    "        )\n",
    "\n",
    "        # Process for parameters\n",
    "        self.param_fc = nn.Sequential(\n",
    "            # Need to take (batchsize, 4) and make (batchsize, 256) for concatenation,\n",
    "            # add hidden layer so that we can infer information about parameters as well.\n",
    "            nn.Linear(4,128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "        # Process for modes\n",
    "        self.cond_fc = nn.Sequential(\n",
    "            # Rescales (batchsize, 4->256), maps to same feature space as image and params\n",
    "            nn.Linear(8, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Full combined model for all processes\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4096 + 256 + 256, 512), # image + params + cond\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, params, cond):\n",
    "        img_feat = self.image_conv(img)\n",
    "        param_feat = self.param_fc(params)\n",
    "        cond_feat = self.cond_fc(cond)\n",
    "\n",
    "        x = torch.cat([img_feat, param_feat, cond_feat], dim=1)\n",
    "        final = self.model(x)\n",
    "\n",
    "        return final.squeeze() # returns (batchsize), where each number is 0 -> 1 based on how likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator for conditional GAN\n",
    "    Inputs:\n",
    "        - Modes (condition), size (batchsize, 4)\n",
    "    Outputs:\n",
    "        - Waveguide, size (batchsize, 32, 32)\n",
    "        - Params, size (batchsize, 4)\n",
    "    Questions:\n",
    "        - Should we still be using latent vector like in MNIST, as we want \n",
    "          consistent results i.e. for a set of modes, we want as close \n",
    "          to the same waveguide as possible each time? \n",
    "        - Should I be feeding my generated waveguide shape into my params\n",
    "          process as well (and maybe in discrim too)? Also, does my params\n",
    "          process need more layers?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            # Need to transform cond vector into higher dimension\n",
    "            # so that we can reshape it for deconv (batchsize, 8 -> 4096)\n",
    "            nn.Linear(8, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(inplace=True) \n",
    "        )\n",
    "        # Output = (input_size-1)*stride-2*padding+kernel_size\n",
    "        self.deconv = nn.Sequential(\n",
    "            # We start with 256 4x4 pieces generated from our cond input\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 4x4 ->  8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), # 16x16 -> 32x32, greyscale so only 1 output channel\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Takes in cond and outputs parameters\n",
    "        \n",
    "        self.param_proc = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 4) # outputs (batchsize, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        x = self.fc(cond) # (batchsize, 8 -> 4096)\n",
    "        cond_feat = x.view(x.size(0), 256, 4, 4) #(batchsize, 4096) -> (batchsize, 256, 4, 4)\n",
    "        image = self.deconv(cond_feat)\n",
    "        params = self.param_proc(x)\n",
    "\n",
    "        return image, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch, discriminator, generator, g_optimizer, g_criterion, d_criterion, device, adv_w=1, re_w=1):\n",
    "    generator.train()\n",
    "    g_optimizer.zero_grad()\n",
    "    # will almost certainly have to change but same logic flow\n",
    "    eigenmodes, weights, real_params, real_waveguides = [b.to(device) for b in batch]\n",
    "    cond = torch.cat([eigenmodes, weights], dim=-1)\n",
    "\n",
    "    fake_waveguides, fake_params = generator(cond)\n",
    "\n",
    "    validity = discriminator(fake_waveguides, fake_params, cond)\n",
    "    adv_loss = d_criterion(validity, Variable(torch.ones_like(validity))) # how it fairs against discriminator\n",
    "\n",
    "    # These are how it fairs against real data, included because only one real result, unsure if to keep? \n",
    "    image_loss = g_criterion(fake_waveguides, real_waveguides)\n",
    "    params_loss = g_criterion(fake_params, real_params)\n",
    "\n",
    "    # can adjust weights to make it fully adversarial \n",
    "    g_loss = adv_loss * adv_w + (image_loss + params_loss) * re_w\n",
    "\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch, discriminator, generator, d_optimizer, d_criterion, device):\n",
    "    discriminator.train()\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    eigenmodes, weights, real_params, real_waveguides = [b.to(device) for b in batch]\n",
    "    cond = torch.cat([eigenmodes, weights], dim=-1)\n",
    "\n",
    "    real_validity = discriminator(real_waveguides, real_params, cond)\n",
    "    real_loss = d_criterion(real_validity, Variable(torch.ones_like(real_validity)))\n",
    "\n",
    "    fake_waveguides, fake_params = generator(cond)\n",
    "    fake_validity = discriminator(fake_waveguides, fake_params, cond)\n",
    "    fake_loss = d_criterion(fake_validity, Variable(torch.zeros_like(real_validity)))\n",
    "\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out differences between my dataset structure and MNIST dataset structure\n",
    "\n",
    "Need to implement training loop, remember that output must be binarized before being fed to the discriminator!\n",
    "\n",
    "For binarization, will that not significantly increase the loss of my model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_critic):\n\u001b[0;32m---> 23\u001b[0m     d_loss \u001b[39m=\u001b[39m discriminator_train_step(batch, d,g, d_optimizer, d_criterion, device)\n\u001b[1;32m     25\u001b[0m g_loss \u001b[39m=\u001b[39m generator_train_step(batch, d, g, g_optimizer, g_criterion, d_criterion, device)\n\u001b[1;32m     26\u001b[0m writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m'\u001b[39m\u001b[39mscalars\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mg_loss\u001b[39m\u001b[39m'\u001b[39m: g_loss, \u001b[39m'\u001b[39m\u001b[39md_loss\u001b[39m\u001b[39m'\u001b[39m: (d_loss \u001b[39m/\u001b[39m n_critic)}, step)  \n",
      "Cell \u001b[0;32mIn[110], line 12\u001b[0m, in \u001b[0;36mdiscriminator_train_step\u001b[0;34m(batch, discriminator, generator, d_optimizer, d_criterion, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m real_loss \u001b[39m=\u001b[39m d_criterion(real_validity, Variable(torch\u001b[39m.\u001b[39mones_like(real_validity)))\n\u001b[1;32m     11\u001b[0m fake_waveguides, fake_params \u001b[39m=\u001b[39m generator(cond)\n\u001b[0;32m---> 12\u001b[0m fake_validity \u001b[39m=\u001b[39m discriminator(fake_waveguides, fake_params, cond)\n\u001b[1;32m     13\u001b[0m fake_loss \u001b[39m=\u001b[39m d_criterion(fake_validity, Variable(torch\u001b[39m.\u001b[39mzeros_like(real_validity)))\n\u001b[1;32m     15\u001b[0m d_loss \u001b[39m=\u001b[39m real_loss \u001b[39m+\u001b[39m fake_loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[107], line 57\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, img, params, cond)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img, params, cond):\n\u001b[0;32m---> 57\u001b[0m     img_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_conv(img)\n\u001b[1;32m     58\u001b[0m     param_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_fc(params)\n\u001b[1;32m     59\u001b[0m     cond_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcond_fc(cond)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # set up device\n",
    "\n",
    "d = Discriminator().to(device)\n",
    "g = Generator().to(device) \n",
    "d_optimizer = torch.optim.Adam(d.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(g.parameters(), lr=1e-4)\n",
    "d_criterion = nn.BCELoss() # outputs [0,1]\n",
    "g_criterion = nn.MSELoss() # outputs [-1,1]\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_epochs = 50\n",
    "n_critic = 5\n",
    "display_step = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    print('Starting epoch {}...'.format(epoch), end=' ')\n",
    "    i = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        step = epoch * len(dataloader) + i + 1\n",
    "        i += 1\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "            d_loss = discriminator_train_step(batch, d,g, d_optimizer, d_criterion, device)\n",
    "        \n",
    "        g_loss = generator_train_step(batch, d, g, g_optimizer, g_criterion, d_criterion, device)\n",
    "        writer.add_scalars('scalars', {'g_loss': g_loss, 'd_loss': (d_loss / n_critic)}, step)  \n",
    "\n",
    "        if step % display_step == 0:\n",
    "            g.eval()\n",
    "            batch = [dataset[i] for i in range(10)]\n",
    "            e_modes, weights, params, real_wguides = zip(*batch)\n",
    "\n",
    "            e_modes = torch.stack(e_modes).to(device)          # (10, 4)\n",
    "            weights = torch.stack(weights).to(device)          # (10, 4)\n",
    "            cond = torch.cat([e_modes, weights], dim=1)        # (10, 8)\n",
    "\n",
    "            real_wguides = torch.stack(real_wguides).to(device)  # (10, 1, 32, 32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_wguides, _ = g(cond)                       # (10, 1, 32, 32)\n",
    "            grid_fake = make_grid(fake_wguides, nrow=5, normalize=True)\n",
    "            grid_real = make_grid(real_wguides, nrow=5, normalize=True)\n",
    "\n",
    "            # Write to TensorBoard\n",
    "            writer.add_image('Generated_Waveguides', grid_fake, step)\n",
    "            writer.add_image('Real_Waveguides', grid_real, step)\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f'Done! - {elapsed} s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
