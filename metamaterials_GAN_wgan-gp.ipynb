{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "# useful v1 functions\n",
    "import import_ipynb \n",
    "from metamaterials_GAN_v1 import plot_shape, load_item, quarter, dataset, dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# Generator (unchanged)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 128), nn.ReLU(True),\n",
    "            nn.Linear(128, 512),       nn.ReLU(True),\n",
    "        )\n",
    "        self.fc_img = nn.Linear(512, 64 * 4 * 4)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16,  1, 4, 2, 1), nn.Sigmoid(),\n",
    "        )\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(512, 32), nn.ReLU(True),\n",
    "            nn.Linear(32, 4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        x = self.fc(cond)\n",
    "        img_feats = self.fc_img(x).view(-1, 64, 4, 4)\n",
    "        waveguide = self.deconv(img_feats)\n",
    "        params    = self.fc_params(x)\n",
    "        return waveguide, params\n",
    "\n",
    "\n",
    "# Critic (WGAN-GP)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        super(Critic, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(16,32, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(32,64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.fc_image = nn.Linear(64*4*4, 128)\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(4, 16), nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.fc_cond = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 16), nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        # No Sigmoid: output an unconstrained scalar\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(128 + 16 + 16, 64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveguide, params, cond):\n",
    "        bsz  = waveguide.size(0)\n",
    "        x_img = self.cnn(waveguide).view(bsz, -1)\n",
    "        x_img = self.fc_image(x_img)\n",
    "        x_p   = self.fc_params(params)\n",
    "        x_c   = self.fc_cond(cond)\n",
    "        x     = torch.cat([x_img, x_p, x_c], dim=1)\n",
    "        return self.fc_final(x)\n",
    "\n",
    "\n",
    "# Gradient Penalty Helper\n",
    "def compute_gradient_penalty(critic, real_imgs, fake_imgs, real_params, cond, device, λ_gp=10.0):\n",
    "    bsz = real_imgs.size(0)\n",
    "    # interpolation factor\n",
    "    α = torch.rand(bsz, 1, 1, 1, device=device)\n",
    "    interpolates = (α * real_imgs + (1 - α) * fake_imgs).requires_grad_(True)\n",
    "    # critic output on interpolated samples\n",
    "    interp_scores = critic(interpolates, real_params, cond)\n",
    "    # gradients of scores w.r.t. interpolates\n",
    "    grads = autograd.grad(\n",
    "        outputs=interp_scores,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(interp_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    # gradient penalty: (||∇||₂ − 1)²\n",
    "    gp = λ_gp * ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Models\n",
    "generator = Generator(cond_dim=8).to(device)\n",
    "critic    = Critic(cond_dim=8).to(device)\n",
    "\n",
    "# Hyper‑parameters\n",
    "lr         = 1e-4        # slightly higher than RMSprop WGAN\n",
    "n_critic   = 5           # critic steps per generator step\n",
    "lambda_gp  = 10.0        # gradient penalty weight\n",
    "\n",
    "# Optimizers (WGAN‑GP uses Adam safely)\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizer_c = optim.Adam(critic.parameters(),    lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib\n",
    "    matplotlib.use('TkAgg')  # Or 'QtAgg', 'WXAgg'\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # — Hyperparameters —\n",
    "    epochs       = 100\n",
    "    lambda_param = 10.0    # weight for parameter‐MSE term\n",
    "\n",
    "    # — Models (assumes Generator, Critic, compute_gradient_penalty are defined above) —\n",
    "    generator = Generator(cond_dim=8).to(device)\n",
    "    critic    = Critic(cond_dim=8).to(device)\n",
    "\n",
    "    # — Loss for the parameter regression branch —\n",
    "    criterion_param = nn.MSELoss()\n",
    "\n",
    "    # — Optimizers (WGAN-GP typically uses Adam) —\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    optimizer_c = optim.Adam(critic.parameters(),    lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    # — Tracking losses —\n",
    "    train_c_losses = []\n",
    "    train_g_losses = []\n",
    "\n",
    "    # — Interactive plotting setup —\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.perf_counter()\n",
    "        c_epoch = []\n",
    "        g_epoch = []\n",
    "\n",
    "        for em, wts, prm, real_wg in dataloader:\n",
    "            # Move data to device\n",
    "            em, wts, prm = em.to(device), wts.to(device), prm.to(device)\n",
    "            real_wg      = real_wg.to(device).unsqueeze(1)  # (B,1,32,32)\n",
    "            cond         = torch.cat([em, wts], dim=1)\n",
    "            bsz          = em.size(0)\n",
    "\n",
    "            # — Train Critic n_critic times —\n",
    "            for _ in range(n_critic):\n",
    "                critic.zero_grad()\n",
    "                # Real + fake scores\n",
    "                fake_wg, fake_prm = generator(cond)\n",
    "                real_score = critic(real_wg,    prm, cond).mean()\n",
    "                fake_score = critic(fake_wg.detach(), fake_prm.detach(), cond).mean()\n",
    "                # Gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    critic, real_wg, fake_wg.detach(), prm, cond, device, λ_gp=lambda_gp\n",
    "                )\n",
    "                # Critic loss\n",
    "                c_loss = fake_score - real_score + gp\n",
    "                c_loss.backward()\n",
    "                optimizer_c.step()\n",
    "\n",
    "            c_epoch.append(c_loss.item())\n",
    "\n",
    "            # — Train Generator —\n",
    "            generator.zero_grad()\n",
    "            fake_wg2, fake_prm2 = generator(cond)\n",
    "            # Adversarial loss\n",
    "            g_adv   = -critic(fake_wg2, fake_prm2, cond).mean()\n",
    "            # Param regression loss\n",
    "            g_param = criterion_param(fake_prm2, prm)\n",
    "            g_loss  = g_adv + lambda_param * g_param\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            g_epoch.append(g_loss.item())\n",
    "\n",
    "        # — Epoch logging —\n",
    "        avg_c  = np.mean(c_epoch)\n",
    "        avg_g  = np.mean(g_epoch)\n",
    "        train_c_losses.append(avg_c)\n",
    "        train_g_losses.append(avg_g)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        print(f\"Epoch {epoch:3d}/{epochs} — Critic: {avg_c:.4f}, Generator: {avg_g:.4f} — {elapsed:.1f}s\")\n",
    "\n",
    "        # — Update live plot —\n",
    "        ax.clear()\n",
    "        ax.plot(train_c_losses, label='Train Critic Loss')\n",
    "        ax.plot(train_g_losses, label='Train Generator Loss')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('WGAN‑GP Losses (Live)')\n",
    "        ax.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "    # — Save the generator —\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    save_path = \"./models/generator_wgan_gp.pth\"\n",
    "    torch.save(generator.state_dict(), save_path)\n",
    "    print(f\"Generator state_dict saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waveguide(generator, eigenmodes_weights):\n",
    "    \"\"\"\n",
    "    Given a trained WGAN generator and a flat vector of 8 cond features\n",
    "    (4 eigenmodes + 4 weights), returns the generated waveguide and params.\n",
    "    \"\"\"\n",
    "    device = next(generator.parameters()).device\n",
    "    x = (\n",
    "        torch.tensor(eigenmodes_weights, dtype=torch.float32)\n",
    "             .unsqueeze(0)\n",
    "             .to(device)\n",
    "    )\n",
    "\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        waveguide, params = generator(x)\n",
    "    return waveguide, params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gen = Generator(cond_dim=8).to(device)\n",
    "    gen.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    \n",
    "    test_vals = load_item(dataset[1], train=False)\n",
    "    cond = np.concatenate((test_vals[\"Eigenmodes\"], test_vals[\"Weights\"]))\n",
    "\n",
    "    gen_waveguide, gen_params = generate_waveguide(gen, cond)\n",
    "    gen_waveguide = (gen_waveguide >= 0.5).float()\n",
    "    wg = gen_waveguide.squeeze().cpu().numpy()\n",
    "\n",
    "    ## Eye test evaluate \n",
    "    plot_shape(wg)\n",
    "    print(f\"Conditions:       {cond}\")\n",
    "    print(f\"Real Parameters:   {test_vals['Params']}\")\n",
    "    print(f\"Generated Params:  {gen_params.squeeze().cpu().numpy()}\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
