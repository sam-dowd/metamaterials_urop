{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "## Enforce 4-fold symmetry\n",
    "## Give only 1/4 of waveguide\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_shape(shape_matrix):\n",
    "#     \"\"\"Plot the generated shape.\"\"\"\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(6,6))\n",
    "#     ax.set_facecolor('#301934')\n",
    "#     plot_matrix = shape_matrix\n",
    "#     ax.imshow(plot_matrix, origin='upper')\n",
    "#     plt.axis('off')\n",
    "#     print(f'size: {len(plot_matrix)} x {len(plot_matrix[0])}')\n",
    "#     plt.show()\n",
    "\n",
    "def plot_shape(shape_matrix):\n",
    "    \"\"\"Plot the generated shape (expects input shape (1, 32, 32) or (32, 32)).\"\"\"\n",
    "    # Squeeze channel if present\n",
    "    if shape_matrix.ndim == 3 and shape_matrix.shape[0] == 1:\n",
    "        shape_matrix = shape_matrix.squeeze(0)  # → (32, 32)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "    ax.set_facecolor('#301934')\n",
    "    ax.imshow(shape_matrix, origin='upper', cmap='viridis')  # add colormap if needed\n",
    "    plt.axis('off')\n",
    "    # print(f'size: {shape_matrix.shape[0]} x {shape_matrix.shape[1]}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_item(item, p= True, action=''):\n",
    "    if action=='':\n",
    "        if p:\n",
    "            print(f'Eigenmodes: {item[0]}')\n",
    "            print(f'Weights: {item[1]}')\n",
    "            print(f'Params: {item[2]}')\n",
    "        plot_shape(item[3])\n",
    "        return {'Eigenmodes':item[0], 'Weights':item[1], 'Params':item[2]}\n",
    "    if action == 'shape':\n",
    "        return item[3]\n",
    "    \n",
    "def quarter(matrix):\n",
    "    return matrix[:32, :32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenmodes: tensor([4.9921, 1.5929, 0.5911, 0.0000])\n",
      "Weights: tensor([7.7262e+01, 1.4055e+01, 8.6509e+00, 2.6938e-05])\n",
      "Params: tensor([1.3050, 0.6800, 3.3900, 7.9300])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1098910, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "class WaveguideDataset(Dataset):\n",
    "    def __init__(self, h5_file):\n",
    "        self.h5_file = h5py.File(h5_file, 'r')\n",
    "        weights = self.h5_file['weight_train'][:]  # Shape: (N, 4)\n",
    "        weight_sums = np.sum(weights, axis=1)  # Shape: (N,)\n",
    "        patterns = self.h5_file['pattern_train'][:] # Shape: (N, 64, 64)\n",
    "        mask = weight_sums < 100 # Mask that sorts for just good data \n",
    "\n",
    "        self.eigenmodes = torch.tensor(self.h5_file['neff_train'][:][mask], dtype=torch.float32)   # (N, 4)\n",
    "        self.weights = torch.tensor(weights[mask], dtype=torch.float32)                            # (N, 4)\n",
    "        self.paramss = torch.tensor(self.h5_file['params_train'][:][mask], dtype=torch.float32)    # (N, 4)\n",
    "        self.waveguides = torch.tensor(np.array([quarter(p) for p in patterns])[mask], dtype=torch.float32).unsqueeze(1)  # (N, 32, 32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveguides)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eigenmode = self.eigenmodes[idx]  # (10, H, W)\n",
    "        weight = self.weights[idx]  # (10,)\n",
    "        params = self.paramss[idx]\n",
    "        waveguide = self.waveguides[idx]  # (H, W)\n",
    "\n",
    "        return eigenmode, weight, params, waveguide\n",
    "\n",
    "dataset = WaveguideDataset('train_test_split.h5')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# print(dataloader)\n",
    "if __name__ == \"__main__\":\n",
    "    load_item(dataset.__getitem__(2))\n",
    "    print(dataset.waveguides.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake with Parameters included in Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator output:\n",
      "  waveguide shape: torch.Size([16, 1, 32, 32])\n",
      "  params shape: torch.Size([16, 4])\n",
      "Discriminator output shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Generator that maps an input condition (eigenmodes and weights)\n",
    "        to a waveguide image (32x32) and a set of 4 parameters.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # First, process the condition through a fully connected network.\n",
    "        # This \"embedding\" is used both to produce the image and the extra parameters.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # For the waveguide branch, map the 512-dimensional feature vector to a feature map:\n",
    "        # We choose 64 channels with a spatial size of 4x4 (64*4*4 = 1024 features).\n",
    "        self.fc_img = nn.Linear(512, 64 * 4 * 4)\n",
    "        \n",
    "        # Then use a series of ConvTranspose2d layers to upscale to 32x32.\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Upsample from 4x4 to 8x8\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 8x8 to 16x16\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 16x16 to 32x32; output 1 channel for the binary image.\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Ensures the output is in the range [0,1].\n",
    "        )\n",
    "        \n",
    "        # A branch for predicting the extra four parameters.\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 4)  # Output: [wavelength, lattice, n_atom, n_lattice]\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Generator.\n",
    "        \n",
    "        Args:\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) representing the four eigenmodes and four weights.\n",
    "        \n",
    "        Returns:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32) representing the waveguide image.\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4) representing the additional parameters.\n",
    "        \"\"\"\n",
    "        x = self.fc(cond)  # Process condition into a 512-dim feature vector.\n",
    "        \n",
    "        # Generate image: \n",
    "        img_features = self.fc_img(x)\n",
    "        # Reshape to (batch_size, 64, 4, 4)\n",
    "        img_features = img_features.view(-1, 64, 4, 4)\n",
    "        waveguide = self.deconv(img_features)\n",
    "        #waveguide = (waveguide >= 0.5).float()\n",
    "        # Generate the extra parameters via a separate branch.\n",
    "        params = self.fc_params(x)\n",
    "        \n",
    "        return waveguide, params\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Discriminator that judges whether a given tuple (waveguide image, extra parameters, and condition)\n",
    "        comes from the data distribution or from the generator.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Convolutional branch to process the waveguide image (assumed to have shape (1, 32, 32)).\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),  # Output: (16, 16, 16)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1), # Output: (32, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # Output: (64, 4, 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to further process flattened image features.\n",
    "        self.fc_image = nn.Linear(64 * 4 * 4, 128)\n",
    "        \n",
    "        # Process the extra parameters (4-D vector).\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Process the condition input (8-D vector).\n",
    "        self.fc_cond = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combine all features to produce the final decision.\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(128 + 16 + 16, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Outputs a probability.\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveguide, params, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Discriminator.\n",
    "        \n",
    "        Args:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32).\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4).\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) with the eigenmodes/weights.\n",
    "        \n",
    "        Returns:\n",
    "            validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\n",
    "        \"\"\"\n",
    "        batch_size = waveguide.size(0)\n",
    "        x_img = self.cnn(waveguide)\n",
    "        # Flatten image features.\n",
    "        x_img = x_img.view(batch_size, -1)\n",
    "        x_img = self.fc_image(x_img)\n",
    "        \n",
    "        # Embed extra parameters.\n",
    "        x_params = self.fc_params(params)\n",
    "        \n",
    "        # Embed the condition vector.\n",
    "        x_cond = self.fc_cond(cond)\n",
    "        \n",
    "        # Concatenate the three representations.\n",
    "        x = torch.cat([x_img, x_params, x_cond], dim=1)\n",
    "        validity = self.fc_final(x)\n",
    "        \n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Models and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# generator = Generator().to(device)\n",
    "# discriminator = Discriminator().to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         eigenmodes, weights, params, real_waveguides = eigenmodes.to(device), weights.to(device),  params.to(device), real_waveguides.to(device)\n",
    "\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_d.zero_grad()\n",
    "#         # print(real_waveguides.unsqueeze(1).shape)\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1))\n",
    "#         real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "\n",
    "#         # print(eigenmodes.shape[1])\n",
    "#         # print(weights.shape)\n",
    "#         fake_waveguides = generator(eigenmodes, weights, params)\n",
    "#         # print(fake_waveguides.size)\n",
    "#         fake_outputs = discriminator(fake_waveguides.detach())\n",
    "#         fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_g.zero_grad()\n",
    "#         fake_outputs = discriminator(fake_waveguides)\n",
    "#         g_loss = criterion(fake_outputs, real_labels)  # Want G to fool D\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "#     end_time = time.perf_counter()\n",
    "#     execution_time = end_time - start_time\n",
    "#     print(f\"The Epoch took {execution_time:.4f} seconds to run.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with new Generator and Descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer_d\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m \u001b[39m# Process real data:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Unsqueeze the channel dimension for the waveguide image.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m real_outputs \u001b[39m=\u001b[39m discriminator(real_waveguides\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), params, cond)\n\u001b[1;32m     65\u001b[0m real_loss \u001b[39m=\u001b[39m criterion_adv(real_outputs, real_labels)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Generate fake data with the generator.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, waveguide, params, cond)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mForward pass of Discriminator.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m batch_size \u001b[39m=\u001b[39m waveguide\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m x_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(waveguide)\n\u001b[1;32m    127\u001b[0m \u001b[39m# Flatten image features.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m x_img \u001b[39m=\u001b[39m x_img\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import time\n",
    "\n",
    "# # Assume Generator and Discriminator have been defined previously and imported.\n",
    "# # Generator takes an 8-D condition vector and outputs:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# # Discriminator takes:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# #    - condition (batch_size, 8)\n",
    "# # and outputs a probability.\n",
    "\n",
    "# # Set device and hyperparameters.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# lr = 0.0002\n",
    "# epochs = 100\n",
    "\n",
    "# # Instantiate the networks.\n",
    "# generator = Generator(cond_dim=8).to(device)\n",
    "# discriminator = Discriminator(cond_dim=8).to(device)\n",
    "\n",
    "# # Define adversarial loss.\n",
    "# criterion_adv = nn.BCELoss()\n",
    "\n",
    "# # (Optional) Define a regression loss for the parameters.\n",
    "# criterion_param = nn.MSELoss()\n",
    "\n",
    "# # Optimizers for both networks.\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# # Training loop.\n",
    "# # Gradient N\n",
    "# # Figure out correct enviroment\n",
    "# # Plot loss function during training ( to determine convergence )\n",
    "# # No gradient explosion/vanishment\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "    \n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         # Move data to the appropriate device.\n",
    "#         eigenmodes = eigenmodes.to(device)      # shape: (batch_size, 4)\n",
    "#         weights = weights.to(device)            # shape: (batch_size, 4)\n",
    "#         params = params.to(device)              # shape: (batch_size, 4)\n",
    "#         real_waveguides = real_waveguides.to(device)  # shape: (batch_size, 32, 32)\n",
    "        \n",
    "#         # Build the condition vector: concatenate eigenmodes and weights.\n",
    "#         cond = torch.cat([eigenmodes, weights], dim=1)  # shape: (batch_size, 8)\n",
    "#         print(cond.shape)\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Discriminator\n",
    "#         # -------------------------\n",
    "#         optimizer_d.zero_grad()\n",
    "        \n",
    "#         # Process real data:\n",
    "#         # Unsqueeze the channel dimension for the waveguide image.\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1), params, cond)\n",
    "#         real_loss = criterion_adv(real_outputs, real_labels)\n",
    "        \n",
    "#         # Generate fake data with the generator.\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "#         fake_loss = criterion_adv(fake_outputs, fake_labels)\n",
    "        \n",
    "#         # Combine discriminator losses and update.\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Generator\n",
    "#         # -------------------------\n",
    "#         optimizer_g.zero_grad()\n",
    "        \n",
    "#         # Re-generate fake data (to ensure proper gradients flow).\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "        \n",
    "#         # Adversarial loss: try to have the discriminator label fakes as real.\n",
    "#         g_loss_adv = criterion_adv(fake_outputs, real_labels)\n",
    "        \n",
    "#         # (Optional) Parameter loss: force the predicted parameters to match the ground truth.\n",
    "#         g_loss_param = criterion_param(fake_params, params)\n",
    "#         # A weighting factor can be used to balance the two losses.\n",
    "#         lambda_param = 10.0\n",
    "        \n",
    "#         g_loss = g_loss_adv + lambda_param * g_loss_param\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "    \n",
    "#     end_time = time.perf_counter()\n",
    "#     epoch_time = end_time - start_time\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "#     print(f\"Epoch took {epoch_time:.4f} seconds.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3 \n",
    "New training with plotting to track model\n",
    "As of right now, this does not really work. The model does converge, and the parameter generation is kinda close, but the waveguide is basically noise. Should I now beef up discriminator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 — D: 44.1092, G: 35.3858 — 236.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140707765719744delayed_destroy\"\n",
      "    while executing\n",
      "\"140707765719744delayed_destroy\"\n",
      "    (\"after\" script)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/100 — D: 100.0000, G: 22.2665 — 239.4s\n",
      "Epoch   3/100 — D: 100.0000, G: 21.7632 — 222.1s\n",
      "Epoch   4/100 — D: 100.0000, G: 21.4536 — 240.9s\n",
      "Epoch   5/100 — D: 100.0000, G: 21.1960 — 233.7s\n",
      "Epoch   6/100 — D: 100.0000, G: 21.0083 — 229.2s\n",
      "Epoch   7/100 — D: 100.0000, G: 20.8579 — 229.9s\n",
      "Epoch   8/100 — D: 100.0000, G: 20.7385 — 220.9s\n",
      "Epoch   9/100 — D: 100.0000, G: 20.6338 — 225.1s\n",
      "Epoch  10/100 — D: 100.0000, G: 20.5502 — 221.5s\n",
      "Epoch  11/100 — D: 100.0000, G: 20.4679 — 223.7s\n",
      "Epoch  12/100 — D: 100.0000, G: 20.3967 — 221.7s\n",
      "Epoch  13/100 — D: 100.0000, G: 20.3369 — 221.3s\n",
      "Epoch  14/100 — D: 100.0000, G: 20.2726 — 224.5s\n",
      "Epoch  15/100 — D: 100.0000, G: 20.2226 — 218.9s\n",
      "Epoch  16/100 — D: 100.0000, G: 20.1733 — 222.6s\n",
      "Epoch  17/100 — D: 100.0000, G: 20.1416 — 221.0s\n",
      "Epoch  18/100 — D: 100.0000, G: 20.0998 — 226.0s\n",
      "Epoch  19/100 — D: 100.0000, G: 20.0763 — 222.3s\n",
      "Epoch  20/100 — D: 100.0000, G: 20.0362 — 220.2s\n",
      "Epoch  21/100 — D: 100.0000, G: 20.0101 — 224.0s\n",
      "Epoch  22/100 — D: 100.0000, G: 19.9874 — 222.4s\n",
      "Epoch  23/100 — D: 100.0000, G: 19.9607 — 223.4s\n",
      "Epoch  24/100 — D: 100.0000, G: 19.9365 — 222.3s\n",
      "Epoch  25/100 — D: 100.0000, G: 19.9150 — 223.1s\n",
      "Epoch  26/100 — D: 100.0000, G: 19.8927 — 223.6s\n",
      "Epoch  27/100 — D: 100.0000, G: 19.8816 — 225.2s\n",
      "Epoch  28/100 — D: 100.0000, G: 19.8569 — 223.0s\n",
      "Epoch  29/100 — D: 100.0000, G: 19.8456 — 219.9s\n",
      "Epoch  30/100 — D: 100.0000, G: 19.8298 — 225.4s\n",
      "Epoch  31/100 — D: 100.0000, G: 19.8086 — 221.9s\n",
      "Epoch  32/100 — D: 100.0000, G: 19.7981 — 224.5s\n",
      "Epoch  33/100 — D: 100.0000, G: 19.7766 — 222.6s\n",
      "Epoch  34/100 — D: 100.0000, G: 19.7674 — 221.8s\n",
      "Epoch  35/100 — D: 100.0000, G: 19.7524 — 222.9s\n",
      "Epoch  36/100 — D: 100.0000, G: 19.7392 — 224.6s\n",
      "Epoch  37/100 — D: 100.0000, G: 19.7269 — 224.3s\n",
      "Epoch  38/100 — D: 100.0000, G: 19.7219 — 223.5s\n",
      "Epoch  39/100 — D: 100.0000, G: 19.7151 — 223.3s\n",
      "Epoch  40/100 — D: 100.0000, G: 19.6990 — 224.9s\n",
      "Epoch  41/100 — D: 100.0000, G: 19.6896 — 219.9s\n",
      "Epoch  42/100 — D: 100.0000, G: 19.6805 — 225.6s\n",
      "Epoch  43/100 — D: 100.0000, G: 19.6672 — 222.0s\n",
      "Epoch  44/100 — D: 100.0000, G: 19.6610 — 231.2s\n",
      "Epoch  45/100 — D: 100.0000, G: 19.6531 — 224.3s\n",
      "Epoch  46/100 — D: 100.0000, G: 19.6451 — 219.8s\n",
      "Epoch  47/100 — D: 100.0000, G: 19.6391 — 226.0s\n",
      "Epoch  48/100 — D: 100.0000, G: 19.6311 — 225.1s\n",
      "Epoch  49/100 — D: 100.0000, G: 19.6303 — 224.5s\n",
      "Epoch  50/100 — D: 100.0000, G: 19.6181 — 225.2s\n",
      "Epoch  51/100 — D: 100.0000, G: 19.6067 — 223.5s\n",
      "Epoch  52/100 — D: 100.0000, G: 19.6010 — 223.7s\n",
      "Epoch  53/100 — D: 100.0000, G: 19.6010 — 222.7s\n",
      "Epoch  54/100 — D: 100.0000, G: 19.5886 — 225.8s\n",
      "Epoch  55/100 — D: 100.0000, G: 19.5779 — 221.5s\n",
      "Epoch  56/100 — D: 100.0000, G: 19.5797 — 225.7s\n",
      "Epoch  57/100 — D: 100.0000, G: 19.5782 — 230.7s\n",
      "Epoch  58/100 — D: 100.0000, G: 19.5710 — 232.4s\n",
      "Epoch  59/100 — D: 100.0000, G: 19.5605 — 231.1s\n",
      "Epoch  60/100 — D: 100.0000, G: 19.5515 — 226.4s\n",
      "Epoch  61/100 — D: 100.0000, G: 19.5519 — 232.9s\n",
      "Epoch  62/100 — D: 100.0000, G: 19.5404 — 227.3s\n",
      "Epoch  63/100 — D: 100.0000, G: 19.5402 — 231.5s\n",
      "Epoch  64/100 — D: 100.0000, G: 19.5417 — 228.6s\n",
      "Epoch  65/100 — D: 100.0000, G: 19.5382 — 225.9s\n",
      "Epoch  66/100 — D: 100.0000, G: 19.5318 — 232.2s\n",
      "Epoch  67/100 — D: 100.0000, G: 19.5238 — 249.7s\n",
      "Epoch  68/100 — D: 100.0000, G: 19.5248 — 234.7s\n",
      "Epoch  69/100 — D: 100.0000, G: 19.5131 — 244.4s\n",
      "Epoch  70/100 — D: 100.0000, G: 19.5102 — 241.6s\n",
      "Epoch  71/100 — D: 100.0000, G: 19.5028 — 235.0s\n",
      "Epoch  72/100 — D: 100.0000, G: 19.5024 — 225.9s\n",
      "Epoch  73/100 — D: 100.0000, G: 19.4942 — 235.5s\n",
      "Epoch  74/100 — D: 100.0000, G: 19.4931 — 243.5s\n",
      "Epoch  75/100 — D: 100.0000, G: 19.4862 — 230.8s\n",
      "Epoch  76/100 — D: 100.0000, G: 19.4837 — 233.1s\n",
      "Epoch  77/100 — D: 100.0000, G: 19.4800 — 227.7s\n",
      "Epoch  78/100 — D: 100.0000, G: 19.4768 — 230.4s\n",
      "Epoch  79/100 — D: 100.0000, G: 19.4735 — 231.1s\n",
      "Epoch  80/100 — D: 100.0000, G: 19.4735 — 229.2s\n",
      "Epoch  81/100 — D: 100.0000, G: 19.4666 — 229.6s\n",
      "Epoch  82/100 — D: 100.0000, G: 19.4594 — 230.2s\n",
      "Epoch  83/100 — D: 100.0000, G: 19.4597 — 228.1s\n",
      "Epoch  84/100 — D: 100.0000, G: 19.4545 — 228.5s\n",
      "Epoch  85/100 — D: 100.0000, G: 19.4549 — 235.2s\n",
      "Epoch  86/100 — D: 100.0000, G: 19.4508 — 231.8s\n",
      "Epoch  87/100 — D: 100.0000, G: 19.4491 — 231.7s\n",
      "Epoch  88/100 — D: 100.0000, G: 19.4409 — 232.6s\n",
      "Epoch  89/100 — D: 100.0000, G: 19.4387 — 230.3s\n",
      "Epoch  90/100 — D: 100.0000, G: 19.4314 — 235.1s\n",
      "Epoch  91/100 — D: 100.0000, G: 19.4324 — 234.5s\n",
      "Epoch  92/100 — D: 100.0000, G: 19.4312 — 233.9s\n",
      "Epoch  93/100 — D: 100.0000, G: 19.4254 — 233.7s\n",
      "Epoch  94/100 — D: 100.0000, G: 19.4252 — 230.8s\n",
      "Epoch  95/100 — D: 100.0000, G: 19.4185 — 263.1s\n",
      "Epoch  96/100 — D: 100.0000, G: 19.4179 — 231.3s\n",
      "Epoch  97/100 — D: 100.0000, G: 19.4121 — 235.8s\n",
      "Epoch  98/100 — D: 100.0000, G: 19.4147 — 236.2s\n",
      "Epoch  99/100 — D: 100.0000, G: 19.4109 — 233.6s\n",
      "Epoch 100/100 — D: 100.0000, G: 19.4048 — 232.9s\n",
      "Generator state_dict saved to ./models/generator.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib\n",
    "    matplotlib.use('TkAgg') # Or 'QtAgg', 'WXAgg'\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Assume Generator, Discriminator, dataloader, (and optional val_dataloader) are defined above.\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lr = 2e-4\n",
    "    epochs = 100\n",
    "    lambda_param = 10.0\n",
    "\n",
    "    # Instantiate models and losses\n",
    "    generator     = Generator(cond_dim=8).to(device)\n",
    "    discriminator = Discriminator(cond_dim=8).to(device)\n",
    "    criterion_adv   = nn.BCELoss()\n",
    "    criterion_param = nn.MSELoss()\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # Prepare lists for logging\n",
    "    train_d_losses = []\n",
    "    train_g_losses = []\n",
    "    # val_d_losses   = []\n",
    "    # val_g_losses   = []\n",
    "\n",
    "    # Set up interactive plotting\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.perf_counter()\n",
    "        d_epoch = []\n",
    "        g_epoch = []\n",
    "\n",
    "        for em, wts, prm, real_wg in dataloader:\n",
    "            bs = em.size(0)\n",
    "            real_lbl = torch.ones(bs,1,device=device)\n",
    "            fake_lbl = torch.zeros(bs,1,device=device)\n",
    "            # print('working')\n",
    "            em, wts, prm = em.to(device), wts.to(device), prm.to(device)\n",
    "            real_wg = real_wg.to(device).unsqueeze(1)  # (B,1,32,32)\n",
    "            cond    = torch.cat([em, wts], dim=1)\n",
    "\n",
    "            # — Train Discriminator —\n",
    "            optimizer_d.zero_grad()\n",
    "            real_out = discriminator(real_wg, prm, cond)\n",
    "            d_real   = criterion_adv(real_out, real_lbl)\n",
    "\n",
    "            fake_wg, fake_prm = generator(cond)\n",
    "            fake_wg = (fake_wg >= 0.5).float()\n",
    "            fake_out          = discriminator(fake_wg, fake_prm, cond)\n",
    "            d_fake            = criterion_adv(fake_out, fake_lbl)\n",
    "\n",
    "            d_loss = d_real + d_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # — Train Generator —\n",
    "            optimizer_g.zero_grad()\n",
    "            fake_wg2, fake_prm2 = generator(cond)\n",
    "            fake_wg2 = (fake_wg2 >= 0.5).float()\n",
    "            fake_out2           = discriminator(fake_wg2, fake_prm2, cond)\n",
    "\n",
    "            g_adv   = criterion_adv(fake_out2, real_lbl)\n",
    "            g_param = criterion_param(fake_prm2, prm)\n",
    "            g_loss  = g_adv + lambda_param * g_param\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            d_epoch.append(d_loss.item())\n",
    "            g_epoch.append(g_loss.item())\n",
    "\n",
    "        # Average losses this epoch\n",
    "        avg_d = np.mean(d_epoch)\n",
    "        avg_g = np.mean(g_epoch)\n",
    "        train_d_losses.append(avg_d)\n",
    "        train_g_losses.append(avg_g)\n",
    "\n",
    "\n",
    "        elapsed = time.perf_counter() - start\n",
    "        print(f\"Epoch {epoch:3d}/{epochs} — D: {avg_d:.4f}, G: {avg_g:.4f} — {elapsed:.1f}s\")\n",
    "\n",
    "        # — Update live plot —\n",
    "        ax.clear()\n",
    "        ax.plot(train_d_losses, label='Train D Loss')\n",
    "        ax.plot(train_g_losses, label='Train G Loss')\n",
    "\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('GAN Losses (Live)')\n",
    "        ax.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)  # small pause to render\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "    save_dir = \"./models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, \"generator.pth\")\n",
    "    # saving the parameters\n",
    "    torch.save(generator.state_dict(), save_path)\n",
    "    print(f\"Generator state_dict saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Waveguide Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: [1.9191108e+00 5.2149284e-01 0.0000000e+00 0.0000000e+00 9.4543289e+01\n",
      " 4.5941620e+00 7.6345481e-05 4.4746566e-06]\n",
      "Real Params: tensor([0.5260, 0.7800, 2.6300, 3.2300])\n",
      "Generated Params: [0.9447391 0.609935  3.2301726 7.89287  ]\n"
     ]
    }
   ],
   "source": [
    "def generate_waveguide(generator, eigenmodes_weights):\n",
    "    device = next(generator.parameters()).device\n",
    "    x = torch.tensor(eigenmodes_weights, dtype=torch.float32) \\\n",
    "            .unsqueeze(0) \\\n",
    "            .to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        waveguide, params = generator(x)\n",
    "    return waveguide, params\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    %matplotlib inline\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    gen = Generator(cond_dim=8).to(device)\n",
    "\n",
    "    gen.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    gen.eval()\n",
    "\n",
    "   \n",
    "    test_vals = load_item(dataset.__getitem__(1), False)\n",
    "    cond = np.concatenate((test_vals['Eigenmodes'], test_vals['Weights']))\n",
    "\n",
    "    gen_waveguide, gen_params = generate_waveguide(gen, cond)\n",
    "\n",
    "    # print(gen_waveguide)\n",
    "    gen_waveguide = (gen_waveguide >= 0.51).float()\n",
    "\n",
    "    wg = gen_waveguide.squeeze().cpu().numpy()\n",
    "\n",
    "    plot_shape(wg)\n",
    "\n",
    "    print(f\"Conditions: {cond}\")\n",
    "    print(f\"Real Params: {test_vals['Params']}\")\n",
    "    print(f\"Generated Params: {gen_params.squeeze().cpu().numpy()}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
