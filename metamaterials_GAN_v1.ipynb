{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "## Enforce 4-fold symmetry\n",
    "## Give only 1/4 of waveguide\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shape(shape_matrix):\n",
    "    \"\"\"Plot the generated shape.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(6,6))\n",
    "    ax.set_facecolor('#301934')\n",
    "    ax.imshow(shape_matrix, origin='upper')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def load_item(item, p= True, action=''):\n",
    "    if action=='':\n",
    "        if p:\n",
    "            print(f'Eigenmodes: {item[0]}')\n",
    "            print(f'Weights: {item[1]}')\n",
    "            print(f'Params: {item[2]}')\n",
    "        plot_shape(item[3])\n",
    "        return {'Eigenmodes':item[0], 'Weights':item[1], 'Params':item[2]}\n",
    "    if action == 'shape':\n",
    "        return item[3]\n",
    "    \n",
    "def quarter(matrix):\n",
    "    return matrix[:32, :32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenmodes: tensor([4.9921, 1.5929, 0.5911, 0.0000])\n",
      "Weights: tensor([7.7262e+01, 1.4055e+01, 8.6509e+00, 2.6938e-05])\n",
      "Params: tensor([1.3050, 0.6800, 3.3900, 7.9300])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1098910, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "class WaveguideDataset(Dataset):\n",
    "    def __init__(self, h5_file):\n",
    "        self.h5_file = h5py.File(h5_file, 'r')\n",
    "        weights = self.h5_file['weight_train'][:]  # Shape: (N, 4)\n",
    "        weight_sums = np.sum(weights, axis=1)  # Shape: (N,)\n",
    "        patterns = self.h5_file['pattern_train'][:] # Shape: (N, 64, 64)\n",
    "        mask = weight_sums < 100 # Mask that sorts for just good data \n",
    "\n",
    "        self.eigenmodes = self.h5_file['neff_train'][:]  # Shape: (N, 4)\n",
    "        self.weights = weights[mask]  # Shape: (N, 4)\n",
    "        self.paramss = self.h5_file['params_train'][:][mask]\n",
    "        self.waveguides = np.array([quarter(p) for p in patterns])[mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveguides)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eigenmode = self.eigenmodes[idx]  # (10, H, W)\n",
    "        weight = self.weights[idx]  # (10,)\n",
    "        params = self.paramss[idx]\n",
    "        waveguide = self.waveguides[idx]  # (H, W)\n",
    "        \n",
    "        \n",
    "        # Normalize (optional)\n",
    "        eigenmode = torch.tensor(eigenmode, dtype=torch.float32)\n",
    "        weight = torch.tensor(weight, dtype=torch.float32)\n",
    "        params = torch.tensor(params, dtype=torch.float32)\n",
    "        waveguide = torch.tensor(waveguide, dtype=torch.float32)\n",
    "\n",
    "        return eigenmode, weight, params, waveguide\n",
    "\n",
    "dataset = WaveguideDataset('train_test_split.h5')\n",
    "load_item(dataset.__getitem__(2))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(dataset.waveguides.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake with Parameters included in Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator output:\n",
      "  waveguide shape: torch.Size([16, 1, 32, 32])\n",
      "  params shape: torch.Size([16, 4])\n",
      "Discriminator output shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Generator that maps an input condition (eigenmodes and weights)\n",
    "        to a waveguide image (32x32) and a set of 4 parameters.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # First, process the condition through a fully connected network.\n",
    "        # This \"embedding\" is used both to produce the image and the extra parameters.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # For the waveguide branch, map the 512-dimensional feature vector to a feature map:\n",
    "        # We choose 64 channels with a spatial size of 4x4 (64*4*4 = 1024 features).\n",
    "        self.fc_img = nn.Linear(512, 64 * 4 * 4)\n",
    "        \n",
    "        # Then use a series of ConvTranspose2d layers to upscale to 32x32.\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Upsample from 4x4 to 8x8\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 8x8 to 16x16\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 16x16 to 32x32; output 1 channel for the binary image.\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Ensures the output is in the range [0,1].\n",
    "        )\n",
    "        \n",
    "        # A branch for predicting the extra four parameters.\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 4)  # Output: [wavelength, lattice, n_atom, n_lattice]\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Generator.\n",
    "        \n",
    "        Args:\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) representing the four eigenmodes and four weights.\n",
    "        \n",
    "        Returns:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32) representing the waveguide image.\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4) representing the additional parameters.\n",
    "        \"\"\"\n",
    "        x = self.fc(cond)  # Process condition into a 512-dim feature vector.\n",
    "        \n",
    "        # Generate image: \n",
    "        img_features = self.fc_img(x)\n",
    "        # Reshape to (batch_size, 64, 4, 4)\n",
    "        img_features = img_features.view(-1, 64, 4, 4)\n",
    "        waveguide = self.deconv(img_features)\n",
    "        #waveguide = (waveguide >= 0.5).float()\n",
    "        # Generate the extra parameters via a separate branch.\n",
    "        params = self.fc_params(x)\n",
    "        \n",
    "        return waveguide, params\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Discriminator that judges whether a given tuple (waveguide image, extra parameters, and condition)\n",
    "        comes from the data distribution or from the generator.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Convolutional branch to process the waveguide image (assumed to have shape (1, 32, 32)).\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),  # Output: (16, 16, 16)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1), # Output: (32, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # Output: (64, 4, 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to further process flattened image features.\n",
    "        self.fc_image = nn.Linear(64 * 4 * 4, 128)\n",
    "        \n",
    "        # Process the extra parameters (4-D vector).\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Process the condition input (8-D vector).\n",
    "        self.fc_cond = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combine all features to produce the final decision.\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(128 + 16 + 16, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Outputs a probability.\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveguide, params, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Discriminator.\n",
    "        \n",
    "        Args:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32).\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4).\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) with the eigenmodes/weights.\n",
    "        \n",
    "        Returns:\n",
    "            validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\n",
    "        \"\"\"\n",
    "        batch_size = waveguide.size(0)\n",
    "        x_img = self.cnn(waveguide)\n",
    "        # Flatten image features.\n",
    "        x_img = x_img.view(batch_size, -1)\n",
    "        x_img = self.fc_image(x_img)\n",
    "        \n",
    "        # Embed extra parameters.\n",
    "        x_params = self.fc_params(params)\n",
    "        \n",
    "        # Embed the condition vector.\n",
    "        x_cond = self.fc_cond(cond)\n",
    "        \n",
    "        # Concatenate the three representations.\n",
    "        x = torch.cat([x_img, x_params, x_cond], dim=1)\n",
    "        validity = self.fc_final(x)\n",
    "        \n",
    "        return validity\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose we have a batch size of 16.\n",
    "    batch_size = 16\n",
    "    # Create dummy condition input (4 eigenmodes + 4 weights = 8 features per sample)\n",
    "    cond = torch.randn(batch_size, 8)\n",
    "    \n",
    "    # Initialize the generator and forward propagate.\n",
    "    netG = Generator(cond_dim=8)\n",
    "    fake_waveguide, fake_params = netG(cond)\n",
    "    print(\"Generator output:\")\n",
    "    print(\"  waveguide shape:\", fake_waveguide.shape)  # Should be (16, 1, 32, 32)\n",
    "    print(\"  params shape:\", fake_params.shape)        # Should be (16, 4)\n",
    "    \n",
    "    # Now initialize the discriminator.\n",
    "    netD = Discriminator(cond_dim=8)\n",
    "    # Here, we use the generated outputs along with the same condition.\n",
    "    validity = netD(fake_waveguide, fake_params, cond)\n",
    "    print(\"Discriminator output shape:\", validity.shape)  # Should be (16, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Models and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# generator = Generator().to(device)\n",
    "# discriminator = Discriminator().to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         eigenmodes, weights, params, real_waveguides = eigenmodes.to(device), weights.to(device),  params.to(device), real_waveguides.to(device)\n",
    "\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_d.zero_grad()\n",
    "#         # print(real_waveguides.unsqueeze(1).shape)\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1))\n",
    "#         real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "\n",
    "#         # print(eigenmodes.shape[1])\n",
    "#         # print(weights.shape)\n",
    "#         fake_waveguides = generator(eigenmodes, weights, params)\n",
    "#         # print(fake_waveguides.size)\n",
    "#         fake_outputs = discriminator(fake_waveguides.detach())\n",
    "#         fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_g.zero_grad()\n",
    "#         fake_outputs = discriminator(fake_waveguides)\n",
    "#         g_loss = criterion(fake_outputs, real_labels)  # Want G to fool D\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "#     end_time = time.perf_counter()\n",
    "#     execution_time = end_time - start_time\n",
    "#     print(f\"The Epoch took {execution_time:.4f} seconds to run.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with new Generator and Descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer_d\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m \u001b[39m# Process real data:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Unsqueeze the channel dimension for the waveguide image.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m real_outputs \u001b[39m=\u001b[39m discriminator(real_waveguides\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), params, cond)\n\u001b[1;32m     65\u001b[0m real_loss \u001b[39m=\u001b[39m criterion_adv(real_outputs, real_labels)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Generate fake data with the generator.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, waveguide, params, cond)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mForward pass of Discriminator.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m batch_size \u001b[39m=\u001b[39m waveguide\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m x_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(waveguide)\n\u001b[1;32m    127\u001b[0m \u001b[39m# Flatten image features.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m x_img \u001b[39m=\u001b[39m x_img\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import time\n",
    "\n",
    "# # Assume Generator and Discriminator have been defined previously and imported.\n",
    "# # Generator takes an 8-D condition vector and outputs:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# # Discriminator takes:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# #    - condition (batch_size, 8)\n",
    "# # and outputs a probability.\n",
    "\n",
    "# # Set device and hyperparameters.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# lr = 0.0002\n",
    "# epochs = 100\n",
    "\n",
    "# # Instantiate the networks.\n",
    "# generator = Generator(cond_dim=8).to(device)\n",
    "# discriminator = Discriminator(cond_dim=8).to(device)\n",
    "\n",
    "# # Define adversarial loss.\n",
    "# criterion_adv = nn.BCELoss()\n",
    "\n",
    "# # (Optional) Define a regression loss for the parameters.\n",
    "# criterion_param = nn.MSELoss()\n",
    "\n",
    "# # Optimizers for both networks.\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# # Training loop.\n",
    "# # Gradient N\n",
    "# # Figure out correct enviroment\n",
    "# # Plot loss function during training ( to determine convergence )\n",
    "# # No gradient explosion/vanishment\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "    \n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         # Move data to the appropriate device.\n",
    "#         eigenmodes = eigenmodes.to(device)      # shape: (batch_size, 4)\n",
    "#         weights = weights.to(device)            # shape: (batch_size, 4)\n",
    "#         params = params.to(device)              # shape: (batch_size, 4)\n",
    "#         real_waveguides = real_waveguides.to(device)  # shape: (batch_size, 32, 32)\n",
    "        \n",
    "#         # Build the condition vector: concatenate eigenmodes and weights.\n",
    "#         cond = torch.cat([eigenmodes, weights], dim=1)  # shape: (batch_size, 8)\n",
    "#         print(cond.shape)\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Discriminator\n",
    "#         # -------------------------\n",
    "#         optimizer_d.zero_grad()\n",
    "        \n",
    "#         # Process real data:\n",
    "#         # Unsqueeze the channel dimension for the waveguide image.\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1), params, cond)\n",
    "#         real_loss = criterion_adv(real_outputs, real_labels)\n",
    "        \n",
    "#         # Generate fake data with the generator.\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "#         fake_loss = criterion_adv(fake_outputs, fake_labels)\n",
    "        \n",
    "#         # Combine discriminator losses and update.\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Generator\n",
    "#         # -------------------------\n",
    "#         optimizer_g.zero_grad()\n",
    "        \n",
    "#         # Re-generate fake data (to ensure proper gradients flow).\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "        \n",
    "#         # Adversarial loss: try to have the discriminator label fakes as real.\n",
    "#         g_loss_adv = criterion_adv(fake_outputs, real_labels)\n",
    "        \n",
    "#         # (Optional) Parameter loss: force the predicted parameters to match the ground truth.\n",
    "#         g_loss_param = criterion_param(fake_params, params)\n",
    "#         # A weighting factor can be used to balance the two losses.\n",
    "#         lambda_param = 10.0\n",
    "        \n",
    "#         g_loss = g_loss_adv + lambda_param * g_loss_param\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "    \n",
    "#     end_time = time.perf_counter()\n",
    "#     epoch_time = end_time - start_time\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "#     print(f\"Epoch took {epoch_time:.4f} seconds.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3 \n",
    "New training with plotting to track model\n",
    "As of right now, this does not really work. The model does converge, and the parameter generation is kinda close, but the waveguide is basically noise. Should I now beef up discriminator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 — D: 44.1092, G: 35.3858 — 236.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140707765719744delayed_destroy\"\n",
      "    while executing\n",
      "\"140707765719744delayed_destroy\"\n",
      "    (\"after\" script)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/100 — D: 100.0000, G: 22.2665 — 239.4s\n",
      "Epoch   3/100 — D: 100.0000, G: 21.7632 — 222.1s\n",
      "Epoch   4/100 — D: 100.0000, G: 21.4536 — 240.9s\n",
      "Epoch   5/100 — D: 100.0000, G: 21.1960 — 233.7s\n",
      "Epoch   6/100 — D: 100.0000, G: 21.0083 — 229.2s\n",
      "Epoch   7/100 — D: 100.0000, G: 20.8579 — 229.9s\n",
      "Epoch   8/100 — D: 100.0000, G: 20.7385 — 220.9s\n",
      "Epoch   9/100 — D: 100.0000, G: 20.6338 — 225.1s\n",
      "Epoch  10/100 — D: 100.0000, G: 20.5502 — 221.5s\n",
      "Epoch  11/100 — D: 100.0000, G: 20.4679 — 223.7s\n",
      "Epoch  12/100 — D: 100.0000, G: 20.3967 — 221.7s\n",
      "Epoch  13/100 — D: 100.0000, G: 20.3369 — 221.3s\n",
      "Epoch  14/100 — D: 100.0000, G: 20.2726 — 224.5s\n",
      "Epoch  15/100 — D: 100.0000, G: 20.2226 — 218.9s\n",
      "Epoch  16/100 — D: 100.0000, G: 20.1733 — 222.6s\n",
      "Epoch  17/100 — D: 100.0000, G: 20.1416 — 221.0s\n",
      "Epoch  18/100 — D: 100.0000, G: 20.0998 — 226.0s\n",
      "Epoch  19/100 — D: 100.0000, G: 20.0763 — 222.3s\n",
      "Epoch  20/100 — D: 100.0000, G: 20.0362 — 220.2s\n",
      "Epoch  21/100 — D: 100.0000, G: 20.0101 — 224.0s\n",
      "Epoch  22/100 — D: 100.0000, G: 19.9874 — 222.4s\n",
      "Epoch  23/100 — D: 100.0000, G: 19.9607 — 223.4s\n",
      "Epoch  24/100 — D: 100.0000, G: 19.9365 — 222.3s\n",
      "Epoch  25/100 — D: 100.0000, G: 19.9150 — 223.1s\n",
      "Epoch  26/100 — D: 100.0000, G: 19.8927 — 223.6s\n",
      "Epoch  27/100 — D: 100.0000, G: 19.8816 — 225.2s\n",
      "Epoch  28/100 — D: 100.0000, G: 19.8569 — 223.0s\n",
      "Epoch  29/100 — D: 100.0000, G: 19.8456 — 219.9s\n",
      "Epoch  30/100 — D: 100.0000, G: 19.8298 — 225.4s\n",
      "Epoch  31/100 — D: 100.0000, G: 19.8086 — 221.9s\n",
      "Epoch  32/100 — D: 100.0000, G: 19.7981 — 224.5s\n",
      "Epoch  33/100 — D: 100.0000, G: 19.7766 — 222.6s\n",
      "Epoch  34/100 — D: 100.0000, G: 19.7674 — 221.8s\n",
      "Epoch  35/100 — D: 100.0000, G: 19.7524 — 222.9s\n",
      "Epoch  36/100 — D: 100.0000, G: 19.7392 — 224.6s\n",
      "Epoch  37/100 — D: 100.0000, G: 19.7269 — 224.3s\n",
      "Epoch  38/100 — D: 100.0000, G: 19.7219 — 223.5s\n",
      "Epoch  39/100 — D: 100.0000, G: 19.7151 — 223.3s\n",
      "Epoch  40/100 — D: 100.0000, G: 19.6990 — 224.9s\n",
      "Epoch  41/100 — D: 100.0000, G: 19.6896 — 219.9s\n",
      "Epoch  42/100 — D: 100.0000, G: 19.6805 — 225.6s\n",
      "Epoch  43/100 — D: 100.0000, G: 19.6672 — 222.0s\n",
      "Epoch  44/100 — D: 100.0000, G: 19.6610 — 231.2s\n",
      "Epoch  45/100 — D: 100.0000, G: 19.6531 — 224.3s\n",
      "Epoch  46/100 — D: 100.0000, G: 19.6451 — 219.8s\n",
      "Epoch  47/100 — D: 100.0000, G: 19.6391 — 226.0s\n",
      "Epoch  48/100 — D: 100.0000, G: 19.6311 — 225.1s\n",
      "Epoch  49/100 — D: 100.0000, G: 19.6303 — 224.5s\n",
      "Epoch  50/100 — D: 100.0000, G: 19.6181 — 225.2s\n",
      "Epoch  51/100 — D: 100.0000, G: 19.6067 — 223.5s\n",
      "Epoch  52/100 — D: 100.0000, G: 19.6010 — 223.7s\n",
      "Epoch  53/100 — D: 100.0000, G: 19.6010 — 222.7s\n",
      "Epoch  54/100 — D: 100.0000, G: 19.5886 — 225.8s\n",
      "Epoch  55/100 — D: 100.0000, G: 19.5779 — 221.5s\n",
      "Epoch  56/100 — D: 100.0000, G: 19.5797 — 225.7s\n",
      "Epoch  57/100 — D: 100.0000, G: 19.5782 — 230.7s\n",
      "Epoch  58/100 — D: 100.0000, G: 19.5710 — 232.4s\n",
      "Epoch  59/100 — D: 100.0000, G: 19.5605 — 231.1s\n",
      "Epoch  60/100 — D: 100.0000, G: 19.5515 — 226.4s\n",
      "Epoch  61/100 — D: 100.0000, G: 19.5519 — 232.9s\n",
      "Epoch  62/100 — D: 100.0000, G: 19.5404 — 227.3s\n",
      "Epoch  63/100 — D: 100.0000, G: 19.5402 — 231.5s\n",
      "Epoch  64/100 — D: 100.0000, G: 19.5417 — 228.6s\n",
      "Epoch  65/100 — D: 100.0000, G: 19.5382 — 225.9s\n",
      "Epoch  66/100 — D: 100.0000, G: 19.5318 — 232.2s\n",
      "Epoch  67/100 — D: 100.0000, G: 19.5238 — 249.7s\n",
      "Epoch  68/100 — D: 100.0000, G: 19.5248 — 234.7s\n",
      "Epoch  69/100 — D: 100.0000, G: 19.5131 — 244.4s\n",
      "Epoch  70/100 — D: 100.0000, G: 19.5102 — 241.6s\n",
      "Epoch  71/100 — D: 100.0000, G: 19.5028 — 235.0s\n",
      "Epoch  72/100 — D: 100.0000, G: 19.5024 — 225.9s\n",
      "Epoch  73/100 — D: 100.0000, G: 19.4942 — 235.5s\n",
      "Epoch  74/100 — D: 100.0000, G: 19.4931 — 243.5s\n",
      "Epoch  75/100 — D: 100.0000, G: 19.4862 — 230.8s\n",
      "Epoch  76/100 — D: 100.0000, G: 19.4837 — 233.1s\n",
      "Epoch  77/100 — D: 100.0000, G: 19.4800 — 227.7s\n",
      "Epoch  78/100 — D: 100.0000, G: 19.4768 — 230.4s\n",
      "Epoch  79/100 — D: 100.0000, G: 19.4735 — 231.1s\n",
      "Epoch  80/100 — D: 100.0000, G: 19.4735 — 229.2s\n",
      "Epoch  81/100 — D: 100.0000, G: 19.4666 — 229.6s\n",
      "Epoch  82/100 — D: 100.0000, G: 19.4594 — 230.2s\n",
      "Epoch  83/100 — D: 100.0000, G: 19.4597 — 228.1s\n",
      "Epoch  84/100 — D: 100.0000, G: 19.4545 — 228.5s\n",
      "Epoch  85/100 — D: 100.0000, G: 19.4549 — 235.2s\n",
      "Epoch  86/100 — D: 100.0000, G: 19.4508 — 231.8s\n",
      "Epoch  87/100 — D: 100.0000, G: 19.4491 — 231.7s\n",
      "Epoch  88/100 — D: 100.0000, G: 19.4409 — 232.6s\n",
      "Epoch  89/100 — D: 100.0000, G: 19.4387 — 230.3s\n",
      "Epoch  90/100 — D: 100.0000, G: 19.4314 — 235.1s\n",
      "Epoch  91/100 — D: 100.0000, G: 19.4324 — 234.5s\n",
      "Epoch  92/100 — D: 100.0000, G: 19.4312 — 233.9s\n",
      "Epoch  93/100 — D: 100.0000, G: 19.4254 — 233.7s\n",
      "Epoch  94/100 — D: 100.0000, G: 19.4252 — 230.8s\n",
      "Epoch  95/100 — D: 100.0000, G: 19.4185 — 263.1s\n",
      "Epoch  96/100 — D: 100.0000, G: 19.4179 — 231.3s\n",
      "Epoch  97/100 — D: 100.0000, G: 19.4121 — 235.8s\n",
      "Epoch  98/100 — D: 100.0000, G: 19.4147 — 236.2s\n",
      "Epoch  99/100 — D: 100.0000, G: 19.4109 — 233.6s\n",
      "Epoch 100/100 — D: 100.0000, G: 19.4048 — 232.9s\n",
      "Generator state_dict saved to ./models/generator.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # Or 'QtAgg', 'WXAgg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume Generator, Discriminator, dataloader, (and optional val_dataloader) are defined above.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-4\n",
    "epochs = 100\n",
    "lambda_param = 10.0\n",
    "\n",
    "# Instantiate models and losses\n",
    "generator     = Generator(cond_dim=8).to(device)\n",
    "discriminator = Discriminator(cond_dim=8).to(device)\n",
    "criterion_adv   = nn.BCELoss()\n",
    "criterion_param = nn.MSELoss()\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare lists for logging\n",
    "train_d_losses = []\n",
    "train_g_losses = []\n",
    "# val_d_losses   = []\n",
    "# val_g_losses   = []\n",
    "\n",
    "# Set up interactive plotting\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.perf_counter()\n",
    "    d_epoch = []\n",
    "    g_epoch = []\n",
    "\n",
    "    for em, wts, prm, real_wg in dataloader:\n",
    "        bs = em.size(0)\n",
    "        real_lbl = torch.ones(bs,1,device=device)\n",
    "        fake_lbl = torch.zeros(bs,1,device=device)\n",
    "        # print('working')\n",
    "        em, wts, prm = em.to(device), wts.to(device), prm.to(device)\n",
    "        real_wg = real_wg.to(device).unsqueeze(1)  # (B,1,32,32)\n",
    "        cond    = torch.cat([em, wts], dim=1)\n",
    "\n",
    "        # — Train Discriminator —\n",
    "        optimizer_d.zero_grad()\n",
    "        real_out = discriminator(real_wg, prm, cond)\n",
    "        d_real   = criterion_adv(real_out, real_lbl)\n",
    "\n",
    "        fake_wg, fake_prm = generator(cond)\n",
    "        fake_wg = (fake_wg >= 0.5).float()\n",
    "        fake_out          = discriminator(fake_wg, fake_prm, cond)\n",
    "        d_fake            = criterion_adv(fake_out, fake_lbl)\n",
    "\n",
    "        d_loss = d_real + d_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # — Train Generator —\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_wg2, fake_prm2 = generator(cond)\n",
    "        fake_wg2 = (fake_wg2 >= 0.5).float()\n",
    "        fake_out2           = discriminator(fake_wg2, fake_prm2, cond)\n",
    "\n",
    "        g_adv   = criterion_adv(fake_out2, real_lbl)\n",
    "        g_param = criterion_param(fake_prm2, prm)\n",
    "        g_loss  = g_adv + lambda_param * g_param\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        d_epoch.append(d_loss.item())\n",
    "        g_epoch.append(g_loss.item())\n",
    "\n",
    "    # Average losses this epoch\n",
    "    avg_d = np.mean(d_epoch)\n",
    "    avg_g = np.mean(g_epoch)\n",
    "    train_d_losses.append(avg_d)\n",
    "    train_g_losses.append(avg_g)\n",
    "\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Epoch {epoch:3d}/{epochs} — D: {avg_d:.4f}, G: {avg_g:.4f} — {elapsed:.1f}s\")\n",
    "\n",
    "    # — Update live plot —\n",
    "    ax.clear()\n",
    "    ax.plot(train_d_losses, label='Train D Loss')\n",
    "    ax.plot(train_g_losses, label='Train G Loss')\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('GAN Losses (Live)')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)  # small pause to render\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "save_dir = \"./models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"generator.pth\")\n",
    "# saving the parameters\n",
    "torch.save(generator.state_dict(), save_path)\n",
    "print(f\"Generator state_dict saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Waveguide Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m generated_waveguide, params = generate_waveguide(gen, cond)\n\u001b[32m     21\u001b[39m gen_waveguide_plt = generated_waveguide.squeeze(\u001b[32m0\u001b[39m).squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mplot_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_waveguide_plt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConditions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReal Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_vals[\u001b[33m'\u001b[39m\u001b[33mParams\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mplot_shape\u001b[39m\u001b[34m(shape_matrix)\u001b[39m\n\u001b[32m      3\u001b[39m fig, ax = plt.subplots(nrows=\u001b[32m1\u001b[39m, ncols=\u001b[32m1\u001b[39m,figsize=(\u001b[32m6\u001b[39m,\u001b[32m6\u001b[39m))\n\u001b[32m      4\u001b[39m ax.set_facecolor(\u001b[33m'\u001b[39m\u001b[33m#301934\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mupper\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m plt.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5976\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5974\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5976\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5977\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5979\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/matplotlib/image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/matplotlib/image.py:646\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_normalize_image_array\u001b[39m(A):\n\u001b[32m    642\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[33;03m    Image subclasses.\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     A = \u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m A.dtype != np.uint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.can_cast(A.dtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msame_kind\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    648\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconverted to float\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/matplotlib/cbook.py:684\u001b[39m, in \u001b[36msafe_masked_invalid\u001b[39m\u001b[34m(x, copy)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_masked_invalid\u001b[39m(x, copy=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x.dtype.isnative:\n\u001b[32m    686\u001b[39m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[32m    687\u001b[39m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[32m    688\u001b[39m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[32m    689\u001b[39m         x = x.byteswap(inplace=copy).view(x.dtype.newbyteorder(\u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/metamaterials_urop/.venv/lib/python3.12/site-packages/torch/_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from your_model_file import Generator\n",
    "%matplotlib inline\n",
    "gen = Generator(cond_dim=8)             # use the same init args you trained with\n",
    "\n",
    "gen.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "\n",
    "def generate_waveguide(generator, eigenmodes_weights):\n",
    "    generator.eval()\n",
    "    eigenmodes_weights = torch.tensor(eigenmodes_weights, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_waveguide = generator(eigenmodes_weights)\n",
    "    return generated_waveguide\n",
    "\n",
    "# Example usage\n",
    "test_vals = load_item(dataset.__getitem__(2), False)\n",
    "\n",
    "cond = np.concatenate((test_vals['Eigenmodes'], test_vals['Weights']))\n",
    "\n",
    "generated_waveguide, params = generate_waveguide(gen, cond)\n",
    "\n",
    "gen_waveguide_plt = generated_waveguide.squeeze(0).squeeze(0)\n",
    "plot_shape(gen_waveguide_plt)\n",
    "\n",
    "print(f'Conditions: {cond}')\n",
    "print(f\"Real Parameters: {test_vals['Params']}\")\n",
    "print(f'Generated Parameters: {params.squeeze(0)}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: [7.2825537e+00 8.0940896e-01 4.9717742e-01 0.0000000e+00 4.9394848e+01\n",
      " 5.0592598e+01 8.4273386e-05 4.1860098e-06]\n",
      "Real Params: tensor([0.6620, 0.6500, 1.8600, 9.3200])\n",
      "Generated Params: [ 0.90614897  0.56276774  2.500913   13.470221  ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "#from generator.pth import Generator\n",
    "\n",
    "# 1. Pick your device up‐front\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Instantiate + move model to that device\n",
    "gen = Generator(cond_dim=8).to(device)\n",
    "\n",
    "# 3. Load your weights directly onto the same device\n",
    "gen.load_state_dict(torch.load(save_path, map_location=device))\n",
    "gen.eval()\n",
    "\n",
    "def generate_waveguide(generator, eigenmodes_weights):\n",
    "    # Grab whatever device the model is on\n",
    "    device = next(generator.parameters()).device\n",
    "\n",
    "    # Build your input on that device, too\n",
    "    x = torch.tensor(eigenmodes_weights, dtype=torch.float32) \\\n",
    "             .unsqueeze(0) \\\n",
    "             .to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        waveguide, params = generator(x)\n",
    "    return waveguide, params\n",
    "\n",
    "# Usage:\n",
    "test_vals = load_item(dataset.__getitem__(140), False)\n",
    "cond = np.concatenate((test_vals['Eigenmodes'], test_vals['Weights']))\n",
    "\n",
    "gen_waveguide, gen_params = generate_waveguide(gen, cond)\n",
    "#print(gen_waveguide)\n",
    "gen_waveguide = (gen_waveguide >= 0.51).float()\n",
    "#print(gen_waveguide)\n",
    "# print(gen_waveguide)\n",
    "#gen_waveguide = (gen_waveguide >= 0.05).float()\n",
    "# print(gen_waveguide)\n",
    "# Move back to CPU / NumPy for plotting\n",
    "wg = gen_waveguide.squeeze().cpu().numpy()\n",
    "# print(wg)\n",
    "# wg = (wg > 0.53).astype(np.int_)\n",
    "# print(wg)\n",
    "plot_shape(wg)\n",
    "\n",
    "print(f\"Conditions: {cond}\")\n",
    "print(f\"Real Params: {test_vals['Params']}\")\n",
    "print(f\"Generated Params: {gen_params.squeeze().cpu().numpy()}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
