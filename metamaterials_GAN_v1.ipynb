{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "## Enforce 4-fold symmetry\n",
    "## Give only 1/4 of waveguide\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shape(shape_matrix):\n",
    "    \"\"\"Plot the generated shape.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(6,6))\n",
    "    ax.set_facecolor('#301934')\n",
    "    ax.imshow(shape_matrix, origin='upper')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def load_item(item, p= True, action=''):\n",
    "    if action=='':\n",
    "        if p:\n",
    "            print(f'Eigenmodes: {item[0]}')\n",
    "            print(f'Weights: {item[1]}')\n",
    "            print(f'Params: {item[2]}')\n",
    "        plot_shape(item[3])\n",
    "        return {'Eigenmodes':item[0], 'Weights':item[1], 'Params':item[2]}\n",
    "    if action == 'shape':\n",
    "        return item[3]\n",
    "    \n",
    "def quarter(matrix):\n",
    "    return matrix[:32, :32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenmodes: tensor([4.9921, 1.5929, 0.5911, 0.0000])\n",
      "Weights: tensor([7.7262e+01, 1.4055e+01, 8.6509e+00, 2.6938e-05])\n",
      "Params: tensor([1.3050, 0.6800, 3.3900, 7.9300])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1098910, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "class WaveguideDataset(Dataset):\n",
    "    def __init__(self, h5_file):\n",
    "        self.h5_file = h5py.File(h5_file, 'r')\n",
    "        weights = self.h5_file['weight_train'][:]  # Shape: (N, 4)\n",
    "        weight_sums = np.sum(weights, axis=1)  # Shape: (N,)\n",
    "        patterns = self.h5_file['pattern_train'][:] # Shape: (N, 64, 64)\n",
    "        mask = weight_sums < 100 # Mask that sorts for just good data \n",
    "\n",
    "        self.eigenmodes = self.h5_file['neff_train'][:]  # Shape: (N, 4)\n",
    "        self.weights = weights[mask]  # Shape: (N, 4)\n",
    "        self.paramss = self.h5_file['params_train'][:][mask]\n",
    "        self.waveguides = np.array([quarter(p) for p in patterns])[mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveguides)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eigenmode = self.eigenmodes[idx]  # (10, H, W)\n",
    "        weight = self.weights[idx]  # (10,)\n",
    "        params = self.paramss[idx]\n",
    "        waveguide = self.waveguides[idx]  # (H, W)\n",
    "        \n",
    "        \n",
    "        # Normalize (optional)\n",
    "        eigenmode = torch.tensor(eigenmode, dtype=torch.float32)\n",
    "        weight = torch.tensor(weight, dtype=torch.float32)\n",
    "        params = torch.tensor(params, dtype=torch.float32)\n",
    "        waveguide = torch.tensor(waveguide, dtype=torch.float32)\n",
    "\n",
    "        return eigenmode, weight, params, waveguide\n",
    "\n",
    "dataset = WaveguideDataset('train_test_split.h5')\n",
    "load_item(dataset.__getitem__(2))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(dataset.waveguides.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake with Parameters included in Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator output:\n",
      "  waveguide shape: torch.Size([16, 1, 32, 32])\n",
      "  params shape: torch.Size([16, 4])\n",
      "Discriminator output shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Generator that maps an input condition (eigenmodes and weights)\n",
    "        to a waveguide image (32x32) and a set of 4 parameters.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # First, process the condition through a fully connected network.\n",
    "        # This \"embedding\" is used both to produce the image and the extra parameters.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # For the waveguide branch, map the 512-dimensional feature vector to a feature map:\n",
    "        # We choose 64 channels with a spatial size of 4x4 (64*4*4 = 1024 features).\n",
    "        self.fc_img = nn.Linear(512, 64 * 4 * 4)\n",
    "        \n",
    "        # Then use a series of ConvTranspose2d layers to upscale to 32x32.\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Upsample from 4x4 to 8x8\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 8x8 to 16x16\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample from 16x16 to 32x32; output 1 channel for the binary image.\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Ensures the output is in the range [0,1].\n",
    "        )\n",
    "        \n",
    "        # A branch for predicting the extra four parameters.\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 4)  # Output: [wavelength, lattice, n_atom, n_lattice]\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Generator.\n",
    "        \n",
    "        Args:\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) representing the four eigenmodes and four weights.\n",
    "        \n",
    "        Returns:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32) representing the waveguide image.\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4) representing the additional parameters.\n",
    "        \"\"\"\n",
    "        x = self.fc(cond)  # Process condition into a 512-dim feature vector.\n",
    "        \n",
    "        # Generate image: \n",
    "        img_features = self.fc_img(x)\n",
    "        # Reshape to (batch_size, 64, 4, 4)\n",
    "        img_features = img_features.view(-1, 64, 4, 4)\n",
    "        waveguide = self.deconv(img_features)\n",
    "        waveguide = (waveguide >= 0.5).float()\n",
    "        # Generate the extra parameters via a separate branch.\n",
    "        params = self.fc_params(x)\n",
    "        \n",
    "        return waveguide, params\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, cond_dim=8):\n",
    "        \"\"\"\n",
    "        Discriminator that judges whether a given tuple (waveguide image, extra parameters, and condition)\n",
    "        comes from the data distribution or from the generator.\n",
    "        \n",
    "        Args:\n",
    "            cond_dim (int): Dimension of the condition input (default=8)\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Convolutional branch to process the waveguide image (assumed to have shape (1, 32, 32)).\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),  # Output: (16, 16, 16)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1), # Output: (32, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # Output: (64, 4, 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to further process flattened image features.\n",
    "        self.fc_image = nn.Linear(64 * 4 * 4, 128)\n",
    "        \n",
    "        # Process the extra parameters (4-D vector).\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Process the condition input (8-D vector).\n",
    "        self.fc_cond = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combine all features to produce the final decision.\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(128 + 16 + 16, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Outputs a probability.\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveguide, params, cond):\n",
    "        \"\"\"\n",
    "        Forward pass of Discriminator.\n",
    "        \n",
    "        Args:\n",
    "            waveguide (torch.Tensor): Tensor of shape (batch_size, 1, 32, 32).\n",
    "            params (torch.Tensor): Tensor of shape (batch_size, 4).\n",
    "            cond (torch.Tensor): Tensor of shape (batch_size, 8) with the eigenmodes/weights.\n",
    "        \n",
    "        Returns:\n",
    "            validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\n",
    "        \"\"\"\n",
    "        batch_size = waveguide.size(0)\n",
    "        x_img = self.cnn(waveguide)\n",
    "        # Flatten image features.\n",
    "        x_img = x_img.view(batch_size, -1)\n",
    "        x_img = self.fc_image(x_img)\n",
    "        \n",
    "        # Embed extra parameters.\n",
    "        x_params = self.fc_params(params)\n",
    "        \n",
    "        # Embed the condition vector.\n",
    "        x_cond = self.fc_cond(cond)\n",
    "        \n",
    "        # Concatenate the three representations.\n",
    "        x = torch.cat([x_img, x_params, x_cond], dim=1)\n",
    "        validity = self.fc_final(x)\n",
    "        \n",
    "        return validity\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose we have a batch size of 16.\n",
    "    batch_size = 16\n",
    "    # Create dummy condition input (4 eigenmodes + 4 weights = 8 features per sample)\n",
    "    cond = torch.randn(batch_size, 8)\n",
    "    \n",
    "    # Initialize the generator and forward propagate.\n",
    "    netG = Generator(cond_dim=8)\n",
    "    fake_waveguide, fake_params = netG(cond)\n",
    "    print(\"Generator output:\")\n",
    "    print(\"  waveguide shape:\", fake_waveguide.shape)  # Should be (16, 1, 32, 32)\n",
    "    print(\"  params shape:\", fake_params.shape)        # Should be (16, 4)\n",
    "    \n",
    "    # Now initialize the discriminator.\n",
    "    netD = Discriminator(cond_dim=8)\n",
    "    # Here, we use the generated outputs along with the same condition.\n",
    "    validity = netD(fake_waveguide, fake_params, cond)\n",
    "    print(\"Discriminator output shape:\", validity.shape)  # Should be (16, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Models and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# generator = Generator().to(device)\n",
    "# discriminator = Discriminator().to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         eigenmodes, weights, params, real_waveguides = eigenmodes.to(device), weights.to(device),  params.to(device), real_waveguides.to(device)\n",
    "\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_d.zero_grad()\n",
    "#         # print(real_waveguides.unsqueeze(1).shape)\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1))\n",
    "#         real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "\n",
    "#         # print(eigenmodes.shape[1])\n",
    "#         # print(weights.shape)\n",
    "#         fake_waveguides = generator(eigenmodes, weights, params)\n",
    "#         # print(fake_waveguides.size)\n",
    "#         fake_outputs = discriminator(fake_waveguides.detach())\n",
    "#         fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_g.zero_grad()\n",
    "#         fake_outputs = discriminator(fake_waveguides)\n",
    "#         g_loss = criterion(fake_outputs, real_labels)  # Want G to fool D\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "#     end_time = time.perf_counter()\n",
    "#     execution_time = end_time - start_time\n",
    "#     print(f\"The Epoch took {execution_time:.4f} seconds to run.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with new Generator and Descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer_d\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m \u001b[39m# Process real data:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Unsqueeze the channel dimension for the waveguide image.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m real_outputs \u001b[39m=\u001b[39m discriminator(real_waveguides\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), params, cond)\n\u001b[1;32m     65\u001b[0m real_loss \u001b[39m=\u001b[39m criterion_adv(real_outputs, real_labels)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Generate fake data with the generator.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, waveguide, params, cond)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mForward pass of Discriminator.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    validity (torch.Tensor): Tensor of shape (batch_size, 1) representing the probability of being real.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m batch_size \u001b[39m=\u001b[39m waveguide\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m x_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(waveguide)\n\u001b[1;32m    127\u001b[0m \u001b[39m# Flatten image features.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m x_img \u001b[39m=\u001b[39m x_img\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import time\n",
    "\n",
    "# # Assume Generator and Discriminator have been defined previously and imported.\n",
    "# # Generator takes an 8-D condition vector and outputs:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# # Discriminator takes:\n",
    "# #    - waveguide (batch_size, 1, 32, 32)\n",
    "# #    - parameters (batch_size, 4)\n",
    "# #    - condition (batch_size, 8)\n",
    "# # and outputs a probability.\n",
    "\n",
    "# # Set device and hyperparameters.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# lr = 0.0002\n",
    "# epochs = 100\n",
    "\n",
    "# # Instantiate the networks.\n",
    "# generator = Generator(cond_dim=8).to(device)\n",
    "# discriminator = Discriminator(cond_dim=8).to(device)\n",
    "\n",
    "# # Define adversarial loss.\n",
    "# criterion_adv = nn.BCELoss()\n",
    "\n",
    "# # (Optional) Define a regression loss for the parameters.\n",
    "# criterion_param = nn.MSELoss()\n",
    "\n",
    "# # Optimizers for both networks.\n",
    "# optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# # Training loop.\n",
    "# # Gradient N\n",
    "# # Figure out correct enviroment\n",
    "# # Plot loss function during training ( to determine convergence )\n",
    "# # No gradient explosion/vanishment\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.perf_counter()\n",
    "    \n",
    "#     for eigenmodes, weights, params, real_waveguides in dataloader:\n",
    "#         # Move data to the appropriate device.\n",
    "#         eigenmodes = eigenmodes.to(device)      # shape: (batch_size, 4)\n",
    "#         weights = weights.to(device)            # shape: (batch_size, 4)\n",
    "#         params = params.to(device)              # shape: (batch_size, 4)\n",
    "#         real_waveguides = real_waveguides.to(device)  # shape: (batch_size, 32, 32)\n",
    "        \n",
    "#         # Build the condition vector: concatenate eigenmodes and weights.\n",
    "#         cond = torch.cat([eigenmodes, weights], dim=1)  # shape: (batch_size, 8)\n",
    "#         print(cond.shape)\n",
    "#         batch_size = eigenmodes.size(0)\n",
    "#         real_labels = torch.ones(batch_size, 1).to(device)\n",
    "#         fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Discriminator\n",
    "#         # -------------------------\n",
    "#         optimizer_d.zero_grad()\n",
    "        \n",
    "#         # Process real data:\n",
    "#         # Unsqueeze the channel dimension for the waveguide image.\n",
    "#         real_outputs = discriminator(real_waveguides.unsqueeze(1), params, cond)\n",
    "#         real_loss = criterion_adv(real_outputs, real_labels)\n",
    "        \n",
    "#         # Generate fake data with the generator.\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "#         fake_loss = criterion_adv(fake_outputs, fake_labels)\n",
    "        \n",
    "#         # Combine discriminator losses and update.\n",
    "#         d_loss = real_loss + fake_loss\n",
    "#         d_loss.backward()\n",
    "#         optimizer_d.step()\n",
    "        \n",
    "#         # -------------------------\n",
    "#         # Train Generator\n",
    "#         # -------------------------\n",
    "#         optimizer_g.zero_grad()\n",
    "        \n",
    "#         # Re-generate fake data (to ensure proper gradients flow).\n",
    "#         fake_waveguides, fake_params = generator(cond)\n",
    "#         fake_outputs = discriminator(fake_waveguides, fake_params, cond)\n",
    "        \n",
    "#         # Adversarial loss: try to have the discriminator label fakes as real.\n",
    "#         g_loss_adv = criterion_adv(fake_outputs, real_labels)\n",
    "        \n",
    "#         # (Optional) Parameter loss: force the predicted parameters to match the ground truth.\n",
    "#         g_loss_param = criterion_param(fake_params, params)\n",
    "#         # A weighting factor can be used to balance the two losses.\n",
    "#         lambda_param = 10.0\n",
    "        \n",
    "#         g_loss = g_loss_adv + lambda_param * g_loss_param\n",
    "#         g_loss.backward()\n",
    "#         optimizer_g.step()\n",
    "    \n",
    "#     end_time = time.perf_counter()\n",
    "#     epoch_time = end_time - start_time\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "#     print(f\"Epoch took {epoch_time:.4f} seconds.\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3 \n",
    "New training with plotting to track model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator state_dict saved to ./models/generator.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # Or 'QtAgg', 'WXAgg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume Generator, Discriminator, dataloader, (and optional val_dataloader) are defined above.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-4\n",
    "epochs = 0\n",
    "lambda_param = 10.0\n",
    "\n",
    "# Instantiate models and losses\n",
    "generator     = Generator(cond_dim=8).to(device)\n",
    "discriminator = Discriminator(cond_dim=8).to(device)\n",
    "criterion_adv   = nn.BCELoss()\n",
    "criterion_param = nn.MSELoss()\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare lists for logging\n",
    "train_d_losses = []\n",
    "train_g_losses = []\n",
    "# val_d_losses   = []\n",
    "# val_g_losses   = []\n",
    "\n",
    "# Set up interactive plotting\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.perf_counter()\n",
    "    d_epoch = []\n",
    "    g_epoch = []\n",
    "\n",
    "    for em, wts, prm, real_wg in dataloader:\n",
    "        bs = em.size(0)\n",
    "        real_lbl = torch.ones(bs,1,device=device)\n",
    "        fake_lbl = torch.zeros(bs,1,device=device)\n",
    "        print('working')\n",
    "        em, wts, prm = em.to(device), wts.to(device), prm.to(device)\n",
    "        real_wg = real_wg.to(device).unsqueeze(1)  # (B,1,32,32)\n",
    "        cond    = torch.cat([em, wts], dim=1)\n",
    "\n",
    "        # — Train Discriminator —\n",
    "        optimizer_d.zero_grad()\n",
    "        real_out = discriminator(real_wg, prm, cond)\n",
    "        d_real   = criterion_adv(real_out, real_lbl)\n",
    "\n",
    "        fake_wg, fake_prm = generator(cond)\n",
    "        fake_out          = discriminator(fake_wg, fake_prm, cond)\n",
    "        d_fake            = criterion_adv(fake_out, fake_lbl)\n",
    "\n",
    "        d_loss = d_real + d_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # — Train Generator —\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_wg2, fake_prm2 = generator(cond)\n",
    "        fake_out2           = discriminator(fake_wg2, fake_prm2, cond)\n",
    "\n",
    "        g_adv   = criterion_adv(fake_out2, real_lbl)\n",
    "        g_param = criterion_param(fake_prm2, prm)\n",
    "        g_loss  = g_adv + lambda_param * g_param\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        d_epoch.append(d_loss.item())\n",
    "        g_epoch.append(g_loss.item())\n",
    "\n",
    "    # Average losses this epoch\n",
    "    avg_d = np.mean(d_epoch)\n",
    "    avg_g = np.mean(g_epoch)\n",
    "    train_d_losses.append(avg_d)\n",
    "    train_g_losses.append(avg_g)\n",
    "\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Epoch {epoch:3d}/{epochs} — D: {avg_d:.4f}, G: {avg_g:.4f} — {elapsed:.1f}s\")\n",
    "\n",
    "    # — Update live plot —\n",
    "    ax.clear()\n",
    "    ax.plot(train_d_losses, label='Train D Loss')\n",
    "    ax.plot(train_g_losses, label='Train G Loss')\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('GAN Losses (Live)')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)  # small pause to render\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "save_dir = \"./models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"generator.pth\")\n",
    "# saving the parameters\n",
    "torch.save(generator.state_dict(), save_path)\n",
    "print(f\"Generator state_dict saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Waveguide Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: [4.9920859e+00 1.5928985e+00 5.9111869e-01 0.0000000e+00 7.7262108e+01\n",
      " 1.4054507e+01 8.6509190e+00 2.6937711e-05]\n",
      "Real Parameters: tensor([1.3050, 0.6800, 3.3900, 7.9300])\n",
      "Generated Parameters: tensor([ 2.1440,  1.1468, -0.2059, -0.6306])\n"
     ]
    }
   ],
   "source": [
    "# from your_model_file import Generator\n",
    "%matplotlib inline\n",
    "gen = Generator(cond_dim=8)             # use the same init args you trained with\n",
    "gen.load_state_dict(torch.load(save_path, map_location=\"cpu\"))\n",
    "\n",
    "def generate_waveguide(generator, eigenmodes_weights):\n",
    "    generator.eval()\n",
    "    eigenmodes_weights = torch.tensor(eigenmodes_weights, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_waveguide = generator(eigenmodes_weights)\n",
    "    return generated_waveguide\n",
    "\n",
    "# Example usage\n",
    "test_vals = load_item(dataset.__getitem__(2), False)\n",
    "\n",
    "cond = np.concatenate((test_vals['Eigenmodes'], test_vals['Weights']))\n",
    "\n",
    "generated_waveguide, params = generate_waveguide(gen, cond)\n",
    "\n",
    "gen_waveguide_plt = generated_waveguide.squeeze(0).squeeze(0)\n",
    "plot_shape(gen_waveguide_plt)\n",
    "\n",
    "print(f'Conditions: {cond}')\n",
    "print(f\"Real Parameters: {test_vals['Params']}\")\n",
    "print(f'Generated Parameters: {params.squeeze(0)}')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
