{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:05<00:00, 149.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from waveguide_dataset import WaveguideDataset\n",
    "dataset = WaveguideDataset('train_test_split.h5')\n",
    "\n",
    "# # Define split sizes (e.g., 80% train, 20% test)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "# pbar = tqdm(test_loader)\n",
    "# i=0\n",
    "# for c, p, x in pbar:\n",
    "#     i+=1\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten function so can include in nn.Sequential(...)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Does CNN on input waveguide, concatenates with conditional parameters, then\n",
    "    does fully connected nn to output final tensor of values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # CNN feature extractor for the image\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),  # [B, 1, 32, 32] -> [B, 32, 30, 30]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),  # -> [B, 64, 28, 28]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # -> [B, 64, 14, 14]\n",
    "            nn.Dropout(0.25),\n",
    "            Flatten()                 # -> [B, 64*14*14 = 12544]\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after combining with 4-dim condition\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12544 + 4, 128),  # Concatenate condition\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 8)           # Output 8 continuous values\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_cond):\n",
    "        x = self.cnn(x_img)                 # [B, 12544]\n",
    "        x = torch.cat((x, x_cond), dim=1)   # [B, 12544 + 4]\n",
    "        output = self.fc(x)                 # [B, 8]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for target, p, x in train_loader:\n",
    "        i += 1\n",
    "        target, p, x = target.to(device), p.to(device), x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, p)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "# pbar = tqdm(dataloader)\n",
    "# train(model, device, pbar, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for target, p, x in test_loader:\n",
    "            target, p, x = target.to(device), p.to(device), x.to(device)\n",
    "            output = model(x,p)\n",
    "            test_loss += loss_fn(output, target).item() * x.size(0)\n",
    "            for t, o in zip(target.cpu(), output.cpu()):\n",
    "                if len(samples) < 50:\n",
    "                    samples.append((t.numpy(), o.numpy()))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}\\n')\n",
    "\n",
    "    chosen = random.sample(samples, 8)\n",
    "    index = []\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    for i, (target, output) in enumerate(chosen):\n",
    "        index.append(i)\n",
    "        targets.append(target)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    headers = [' '] + index\n",
    "    row_1 = ['Target'] + targets\n",
    "    row_2 = ['Output'] + outputs\n",
    "    print(tabulate([row_1, row_2], headers=headers, tablefmt='orgtbl'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13737 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 235/13737 [00:18<17:15, 13.04it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     main(dataset)\n",
      "Cell \u001b[0;32mIn[29], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     train(model, device, pbar_train, optimizer, loss_fn)\n\u001b[1;32m     26\u001b[0m     test(model, device, pbar_test, loss_fn)\n\u001b[1;32m     27\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m output \u001b[39m=\u001b[39m model(x, p)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, target)\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(dataset):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    lr = 1e-3\n",
    "    gamma = 0.7\n",
    "    epochs = 15\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4)\n",
    "    pbar_train = tqdm(train_loader)\n",
    "    pbar_test = tqdm(test_loader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch #{epoch}:')\n",
    "        train(model, device, pbar_train, optimizer, loss_fn)\n",
    "        test(model, device, pbar_test, loss_fn)\n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), 'models/mode_cnn.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Name   | sam       | bruh   |\n",
      "|--------+-----------+--------|\n",
      "| Alice  | [1, 2, 3] | bruh   |\n",
      "| Bob    | [1, 3, 4] | sam    |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([['Alice', [1,2,3], 'bruh'], ['Bob', [1,3,4], 'sam']], headers=['Name', 'sam', 'bruh'], tablefmt='orgtbl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
