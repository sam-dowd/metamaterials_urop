{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n",
      "GPU name: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import os\n",
    "# useful v1 functions\n",
    "# import import_ipynb \n",
    "# import importlib\n",
    "# import metamaterials_GAN_v1\n",
    "# importlib.reload(metamaterials_GAN_v1)\n",
    "\n",
    "# from metamaterials_GAN_v1 import plot_shape, load_item, quarter, dataset, dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shape(shape_matrix):\n",
    "    \"\"\"Plot the generated shape (expects input shape (1, 32, 32) or (32, 32)).\"\"\"\n",
    "    # Squeeze channel if present\n",
    "    if shape_matrix.ndim == 3 and shape_matrix.shape[0] == 1:\n",
    "        shape_matrix = shape_matrix.squeeze(0)  # â†’ (32, 32)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "    ax.set_facecolor('#301934')\n",
    "    ax.imshow(shape_matrix, origin='upper', cmap='viridis')  # add colormap if needed\n",
    "    plt.axis('off')\n",
    "    # print(f'size: {shape_matrix.shape[0]} x {shape_matrix.shape[1]}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_item(item, p= True, action=''):\n",
    "    if action=='':\n",
    "        if p:\n",
    "            print(f'Cond: {item[0]}')\n",
    "            print(f'Params: {item[1]}')\n",
    "        plot_shape(item[2])\n",
    "        return {'Cond':item[0], 'Params':item[1]}\n",
    "    if action == 'shape':\n",
    "        return item[3]\n",
    "    \n",
    "def quarter(matrix):\n",
    "    return matrix[:32, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# %matplotlib inline\n",
    "\n",
    "# # class WaveguideDataset(Dataset):\n",
    "# #     def __init__(self, h5_file):\n",
    "# #         self.h5_file = h5py.File(h5_file, 'r')\n",
    "# #         weights = self.h5_file['weight_train'][:]  # Shape: (N, 4)\n",
    "# #         weight_sums = np.sum(weights, axis=1)  # Shape: (N,)\n",
    "# #         patterns = self.h5_file['pattern_train'][:] # Shape: (N, 64, 64)\n",
    "# #         mask = weight_sums < 100 # Mask that sorts for just good data \n",
    "\n",
    "# #         self.eigenmodes = torch.tensor(self.h5_file['neff_train'][:][mask], dtype=torch.float32)   # (N, 4)\n",
    "# #         self.weights = torch.tensor(weights[mask], dtype=torch.float32)  \n",
    "# #                                  # (N, 4)\n",
    "# #         self.paramss = torch.tensor(self.h5_file['params_train'][:][mask], dtype=torch.float32)    # (N, 4)\n",
    "# #         waveguides_01 = torch.tensor(np.array([quarter(p) for p in patterns])[mask], dtype=torch.float32).unsqueeze(1)  # (N, 32, 32)\n",
    "\n",
    "# #         # Possibly try normalizing all parameters, if training doesn't work\n",
    "# #         # self.eigenmodes = (self.eigenmodes - self.eigenmodes.mean(0)) / self.eigenmodes.std(0)\n",
    "# #         # self.weights = (self.weights - self.weights.mean(0)) / self.weights.std(0)\n",
    "# #         # self.paramss = (self.paramss - self.paramss.mean(0)) / self.paramss.std(0)\n",
    "\n",
    "# #         self.waveguides = (waveguides_01-0.5) * 2\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.waveguides)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         eigenmode = self.eigenmodes[idx]  # (10, H, W)\n",
    "# #         weight = self.weights[idx]  # (10,)\n",
    "# #         params = self.paramss[idx]\n",
    "# #         waveguide = self.waveguides[idx]  # (1, H, W)\n",
    "# #         cond = torch.cat((eigenmode, weight), dim=0) # size (8,)\n",
    "\n",
    "# class WaveguideDataset(Dataset):\n",
    "#     def __init__(self, h5_path):\n",
    "#         self.h5_path = h5_path\n",
    "#         self.indices = self._get_valid_indices()\n",
    "\n",
    "#     def _get_valid_indices(self):\n",
    "#         with h5py.File(self.h5_path, 'r') as f:\n",
    "#             weights = f['weight_train'][:]\n",
    "#             weight_sums = np.sum(weights, axis=1)\n",
    "#             return np.where(weight_sums < 100)[0]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.indices)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         with h5py.File(self.h5_path, 'r') as f:\n",
    "#             i = self.indices[idx]\n",
    "#             eigenmode = torch.tensor(f['neff_train'][i], dtype=torch.float32)\n",
    "#             weight = torch.tensor(f['weight_train'][i], dtype=torch.float32)\n",
    "#             param = torch.tensor(f['params_train'][i], dtype=torch.float32)\n",
    "#             pattern = torch.tensor(quarter(f['pattern_train'][i]), dtype=torch.float32).unsqueeze(0)\n",
    "#             condition = torch.cat((eigenmode, weight), dim=0)\n",
    "#         return condition, param, pattern\n",
    "\n",
    "\n",
    "# dataset = WaveguideDataset('train_test_split.h5')\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# # print(dataloader)\n",
    "# if __name__ == \"__main__\":\n",
    "#     load_item(dataset.__getitem__(2))\n",
    "#     # print(dataset.waveguides.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block, for image processing\n",
    "        '''\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDown(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling path for U-Net, reduces spatial resolution while increasing feature depth\n",
    "    Input: Image batch, size (batchsize, 1, 32, 32)\n",
    "    Output: size (batchsize, out_channels, 16, 16)\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(\n",
    "            in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Doubles spatial dimensions, halves feature dimensions\n",
    "        # My channel dimension for image will always be 1, greyscale\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        Doubles spatial size, but decreases channels:\n",
    "        input: 2 vectors of size (binsize, in_channels / 2, h, w) \n",
    "        output: (binsize, outchannels, 2h, 2w)\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        \"\"\"\n",
    "        x is the upsampled features from previous decoder layer\n",
    "        skip is the skip connection from the encoder, same size as x\n",
    "        \"\"\"\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "a = torch.empty(32, 2, 16,16)\n",
    "b = UnetUp(4,1)\n",
    "print(b.forward(a,a).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "class EmbedFC(nn.Module):\n",
    "    \"\"\"\n",
    "    Use FC layer for embedding 1-d metadata, like modes+weights\n",
    "    (putting into higher dimension)\n",
    "    Effectively our conditional\n",
    "    input: Conditional, size (batchsize, input_dim = 4+4)\n",
    "    Output: Higherdimensional tensor, size (batchsize, output_dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "a = torch.empty(32, 8)\n",
    "b = EmbedFC(8, 32)\n",
    "print(b.forward(a).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net style neural network for conditional image generation, conditioning on\n",
    "    both timestep t and a context vector c (modes+weights)\n",
    "\n",
    "    ** Diverges from MNIST Example:\n",
    "        - instead of inputting n_classes, input the length of my\n",
    "          conditional, as is continuous 8\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, n_feat = 256, cond_dim=8):\n",
    "        \"\"\"\n",
    "        in_channels (1 for greyscale), n_feat is base feature size, n_classes is number of labels\n",
    "        \"\"\"\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        # Lifts image channels to n_feat using our residual block\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat) # (batchsize, 1, 32, 32) -> (batchsize, 2, 16, 16)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat) # (batchsize, 2, 16, 16) -> (batchsize, 4, 8, 8)\n",
    "\n",
    "        # reduces 2d feature map down2 into small latent vector of shape\n",
    "        # (batchsize, 2*n_feat, 1, 1) via 7x7 average pooling\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(8), nn.GELU())\n",
    "\n",
    "        # Embeds timestep t and context c into vectors that will later be reshaped and added\n",
    "        # to upsampling path\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(cond_dim, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(cond_dim, 1*n_feat)\n",
    "\n",
    "        # upsamples latent vector (hiddenvec) back to 2d spatial mapping\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 8, 8), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # upsampling (decoder) blocks that fuse upsampled features with skip connections from\n",
    "        # the encoder\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # final processing layer to reduce features back to the original number of channels\n",
    "        # Ideally produces the final denoised image!\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t):\n",
    "        # x is (noisy) image, size (batchsize, in_channels, 32, 32)\n",
    "        # c is context label,size (batchsize, 8)\n",
    "        # t is timestep scalar, size (batchsize, 1)\n",
    "        # Binary mask for whether to apply conditioning \n",
    "        # probably will not need because need conditioning\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2) # pooled latent representation\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        \n",
    "        # embed context, time step, reshapes them for broadcasting in upsampling layers\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "model = ContextUnet(1, 64, 8)\n",
    "x = torch.zeros(32, 1, 32, 32)           # batch of noisy grayscale images\n",
    "c = torch.zeros(32, 8)          # context labels\n",
    "t = torch.rand(32, 1)                    # timestep in [0, 1]\n",
    "\n",
    "output = model.forward(x, c, t)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Precomputes all noise scheduling terms needed for training and sampling\n",
    "    from a denoising diffusion probabilistic model\n",
    "    Uses a sequence of gradually increasing noise level over T timesteps\n",
    "    beta1: starting noise level, O(1e-4)\n",
    "    beta2: final noise level, O(0.02)\n",
    "    T: number of time steps\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1 # noise variance schedule (for every time t in T)\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    # dictionary of schedule terms\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t , signal retention at time step t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t} , noise scaling factor\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t} , cumulative signal retention\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}} , scales clean image during noise\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}} , noise strength\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}} , for reverse diffusion\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2023, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "tensor([[[[-1.6955,  1.0172,  0.7732,  ..., -0.9045,  0.1969,  0.5361],\n",
      "          [-1.2635, -0.3760, -1.4463,  ...,  0.2331, -0.1896,  0.8265],\n",
      "          [-0.6154, -0.5640, -1.2075,  ..., -0.6519,  0.8329,  0.8758],\n",
      "          ...,\n",
      "          [-1.4195,  1.1703, -0.0975,  ..., -1.4958, -0.2324, -0.7763],\n",
      "          [-0.9242,  1.6879,  0.0532,  ..., -0.2861, -0.6671,  0.4401],\n",
      "          [ 0.7158, -1.4513, -1.8248,  ...,  1.0840,  0.8081,  0.3813]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5163, -0.8446,  3.3006,  ..., -1.4235, -0.6086, -0.3226],\n",
      "          [ 0.1030, -0.1125, -0.8215,  ...,  0.7051,  1.2831, -0.7090],\n",
      "          [ 0.1496,  0.0825,  1.7646,  ...,  1.7021,  1.0686, -0.1094],\n",
      "          ...,\n",
      "          [-0.2988, -0.8375, -0.6457,  ...,  1.6991,  0.7553,  1.5659],\n",
      "          [ 0.9597,  0.3375,  0.1725,  ...,  1.3175,  2.7556, -0.2172],\n",
      "          [-1.5730,  0.0200, -1.6792,  ..., -3.4397,  1.1185, -0.1315]]],\n",
      "\n",
      "\n",
      "        [[[-1.9812,  1.3326,  0.8057,  ..., -0.4077, -0.2956,  1.1145],\n",
      "          [-0.5252,  1.9843, -1.1136,  ...,  0.8120, -1.1649, -0.8140],\n",
      "          [ 0.6648,  2.1026, -0.8860,  ...,  0.4791,  1.7890,  0.8487],\n",
      "          ...,\n",
      "          [-2.0452,  0.4079,  1.6499,  ..., -1.1324,  0.0925, -1.0354],\n",
      "          [ 0.9523, -0.4399, -0.8349,  ..., -0.1194, -1.4187,  0.9822],\n",
      "          [ 2.0599,  2.1122, -0.5643,  ..., -1.8809,  0.9176,  0.9570]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9754,  2.1104,  0.6294,  ...,  0.0209, -0.2267, -0.2780],\n",
      "          [-1.4311,  0.3838,  0.5762,  ...,  1.0093,  0.3826,  0.3825],\n",
      "          [ 0.7758, -0.0334,  1.2692,  ..., -1.1793, -0.6321,  0.2090],\n",
      "          ...,\n",
      "          [ 1.4746,  0.8809,  0.5712,  ...,  1.6429, -0.1783, -1.1718],\n",
      "          [-0.4610,  0.3885,  0.7724,  ...,  0.5248,  0.0378, -0.2694],\n",
      "          [-0.7422, -1.7316, -0.1755,  ...,  0.9568,  0.2814,  1.4462]]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model\n",
    "    \"\"\"\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        \"\"\"\n",
    "        betas: tuple (beta1, beta2) for linear noise schedule\n",
    "        n_T: total number of diffusion steps (e.g. 1000)\n",
    "        drop_prob: probability of dropping conditioning (for classifier free guidance)\n",
    "        \"\"\"\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        x: clean image tensor, size (batchsize, 1, 32, 32)\n",
    "        c: conditional vector, size (batchsize, 32)\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        # t ~ Uniform(0, n_T)\n",
    "        # sample a random timestep t for each item in the batch, \n",
    "        # determines how much noise to add\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        # Generates noising image x_t from clean image x\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        # context_mask = torch.bernoulli(\n",
    "        #     torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        # runs x_t, c, t, and context_mask through model, compares predicted noise\n",
    "        # with actual noise using MSE\n",
    "        # return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T))\n",
    "\n",
    "    def sample(self, n_sample, size, device, c_i, guide_w=0.0):\n",
    "        \"\"\"\n",
    "        n_sample: number of images to generate\n",
    "        size: shape of each image, [1,32,32]\n",
    "        guid_w: guidance strength, 0=no guidance, >0=stronger conditioning (what we want)\n",
    "        \"\"\"\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        # x_T ~ N(0, 1), sample initial noise\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # start from pure noise\n",
    "        # context for us just cycles throught the mnist labels\n",
    "\n",
    "        x_i_store = []  # keep track of generated steps in case want to plot something\n",
    "        print()\n",
    "        # Iterate over timesteps in revers (from noise -> image)\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}', end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T], device=device).repeat(n_sample, 1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0 # add noise at all steps except final one\n",
    "\n",
    "            # predict the noise using both conditioned and unconditioned branches\n",
    "            eps = self.nn_model(x_i, c_i, t_is)\n",
    "            # apply classifier-free guidance formula: Ïµ = (1 + w)â‹…Ïµ_cond âˆ’ wâ‹…Ïµ_uncond\n",
    "\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            # save frames for visualization every 20 steps and near the end\n",
    "            if i % 20 == 0 or i == self.n_T or i < 8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "\n",
    "        # returns final denoised image x_i and intermediate steps x_i_store\n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "\n",
    "ddpm = DDPM(model, betas=(1e-4, 0.02), n_T=10, device='cpu')\n",
    "x = torch.zeros(4, 1, 32, 32)\n",
    "c = torch.zeros(4, 8)\n",
    "loss = ddpm.forward(x, c)\n",
    "print(loss)\n",
    "\n",
    "samples, history = ddpm.sample(n_sample=4, size=(1, 32, 32), device='cpu', c_i=c, guide_w=2.0)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "batch_size = 32\n",
    "n_T = 400\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_feat = 128\n",
    "lrate = 1e-4\n",
    "save_model = True\n",
    "save_dir = './data/waveguide_diffusion_outputs/'\n",
    "guide_weights = [0.0, 0.5, 2.0]  # guidance scaling\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Define the model\n",
    "model = ContextUnet(in_channels=1, n_feat=n_feat, cond_dim=8)  # cond_dim = 4 eig + 4 wts\n",
    "ddpm = DDPM(nn_model=model, betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "# loss = ddpm.sample(x,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4293/4293 [00:08<00:00, 505.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = WaveguideDataset('train_test_split.h5')\n",
    "from waveguide_dataset import WaveguideDataset\n",
    "dataset = WaveguideDataset('train_test_split.h5')\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "# dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=5)\n",
    "pbar = tqdm(dataloader)\n",
    "i=0\n",
    "for c, p, x in pbar:\n",
    "    i+=1\n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_waveguide(dataset):\n",
    "\n",
    "    # hardcoding these here\n",
    "    n_epoch = 20\n",
    "    batch_size = 256\n",
    "    n_T = 400 # difusion steps\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # WILL HAVE TO CHANGE THIS to cond_dim = 8\n",
    "    cond_dim = 8\n",
    "\n",
    "    n_feat = 128 # Feature size for U-Net, may have to make 256\n",
    "    lrate = 1e-4\n",
    "    save_model = False\n",
    "    save_dir = './data/diffusion_v1_outputs/'\n",
    "    ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "\n",
    "    # FOR DISPLAY CHECK\n",
    "    ########################################\n",
    "    n_sample = 40  # 5x8, or whatever number you want\n",
    "    condition_list = []\n",
    "    x_real = []\n",
    "    # Pull first n_sample items from dataset, \n",
    "    for i in range(n_sample):\n",
    "        cond_i, _, x_i = dataset[i]  # cond: (8,), x_i: (1, 32, 32)\n",
    "        condition_list.append(cond_i)\n",
    "        x_real.append(x_i)\n",
    "    c_i = torch.stack(condition_list).to(device)  # (n_sample, 8)\n",
    "    x_real = torch.stack(x_real).to(device) \n",
    "    ########################################\n",
    "\n",
    "    ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, cond_dim=cond_dim), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate) # ADAM optimizer for training, keep\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        print(f'epoch {ep}')\n",
    "        ddpm.train()\n",
    "\n",
    "        # linear lrate decay, so training quicker at begining, more fine-tuned at end?\n",
    "        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "        # batch training\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None # For display\n",
    "\n",
    "        for c, p, x in pbar: # loops over batches from dataset\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            loss = ddpm(x, c) # predicts noise from noisy image\n",
    "            loss.backward() # Computes EMA-smoothed loss for live progress display, backprops to update weights\n",
    "\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item() \n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "        \n",
    "        # for eval, save an image of currently generated samples (top rows)\n",
    "        # followed by real images (bottom rows)\n",
    "        ddpm.eval() # evaluates model without gradient\n",
    "        with torch.no_grad():\n",
    "            for w_i, w in enumerate(ws_test):\n",
    "                # WILL HAVE TO CHANGE TO MAKE WORK WITH MY SAMPLING STRATEGY, ie grabbing first 40 waveguides and generating new ones\n",
    "                x_gen, x_gen_store = ddpm.sample(n_sample, (1, 32, 32), device, c_i=c_i, guide_w=w) # samples n_sample generated images, \n",
    "\n",
    "                # append some real images at bottom, order by class also\n",
    "                # x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "                # for k in range(n_classes):\n",
    "                #     for j in range(int(n_sample/n_classes)):\n",
    "                #         try: \n",
    "                #             idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                #         except:\n",
    "                #             idx = 0\n",
    "                #         x_real[k+(j*n_classes)] = x[idx]\n",
    "\n",
    "                x_all = torch.cat([x_gen, x_real], dim=0)\n",
    "                grid = make_grid(x_all*-1 + 1, nrow=8)\n",
    "                save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "                print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "                if ep%5==0 or ep == int(n_epoch-1):\n",
    "                    # create gif of images evolving over time, based on x_gen_store\n",
    "                    fig, axs = plt.subplots(nrows=n_sample // 8, ncols=8,sharex=True,sharey=True,figsize=(8,3))\n",
    "\n",
    "                    def animate_diff(i, x_gen_store):\n",
    "                        print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
    "                        plots = []\n",
    "                        for row in range(int(n_sample // 8)):\n",
    "                            for col in range(8):\n",
    "                                axs[row, col].clear()\n",
    "                                axs[row, col].set_xticks([])\n",
    "                                axs[row, col].set_yticks([])\n",
    "                                # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                                img = -x_gen_store[i, (row*8) + col, 0]\n",
    "                                plots.append(axs[row,col].imshow(img, cmap='gray', vmin=img.min(), vmax=img.max()))\n",
    "                                # plots.append(axs[row, col].imshow(-x_gen_store[i,(row*n_classes)+col,0],cmap='gray',vmin=(-x_gen_store[i]).min(), vmax=(-x_gen_store[i]).max()))\n",
    "                        return plots\n",
    "\n",
    "                    ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])    \n",
    "                    ani.save(save_dir + f\"gif_ep{ep}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "                    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "        # optionally save model\n",
    "\n",
    "        if save_model and ep == int(n_epoch-1):\n",
    "            torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "            print('saved model at ' + save_dir + f\"model_{ep}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks to complete: completely rework training function for my project\n",
    "Maybe start with just generating the waveguides, then move to generating waveguides and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4293 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     train_waveguide(dataset)\n",
      "Cell \u001b[0;32mIn[15], line 55\u001b[0m, in \u001b[0;36mtrain_waveguide\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     53\u001b[0m c \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m loss \u001b[39m=\u001b[39m ddpm(x, c) \u001b[39m# predicts noise from noisy image\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Computes EMA-smoothed loss for live progress display, backprops to update weights\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m loss_ema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     loss_ema \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_waveguide(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "904b27c75b92146183e9f1345c638188bb604f0e4fe123b9be54acbb552124e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
